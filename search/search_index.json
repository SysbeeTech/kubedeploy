{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"Kubedeploy: Your Gateway to Effortless Kubernetes Application Deployment <p>In the fast-paced world of modern software deployment, simplicity and flexibility are paramount. Meet Kubedeploy, your ultimate solution for effortlessly deploying containerized applications to your Kubernetes cluster.</p> <p>Kubedeploy was born out of the need for a straightforward and versatile framework to streamline the deployment of Docker images to Kubernetes clusters.</p> <p>Quick Start  Documentation  Examples </p>"},{"location":"index.html#kubedeploy-in-a-nutshell","title":"Kubedeploy in a nutshell","text":"<p>Kubedeploy is the bridge that brings simplicity and flexibility together in the Kubernetes deployment journey. Whether you're a beginner looking for an easy start or an advanced user seeking precision and control, Kubedeploy empowers you to navigate the complexities of Kubernetes deployment with ease.</p> <p>Deploy smarter, deploy with Kubedeploy.</p> <p>Simple Nginx deployment</p> <pre><code>helm install nginx sysbee/kubedeploy --set image.repository=nginx\n</code></pre> <p>Follow the Quickstart guide for more examples.</p>"},{"location":"index.html#what-is-kubdeploy","title":"What is Kubdeploy","text":"<ul> <li> <p>Generalized Helm Chart: At the core of Kubedeploy is a generalized Helm chart, designed to effortlessly deploy any containerized application into Kubernetes clusters. Say goodbye to the tedious task of crafting complex Kubernetes manifest files or developing custom Helm charts for your applications.</p> </li> <li> <p>Simplicity for Beginners: Kubedeploy is your trusted ally if you're new to Kubernetes. With just two values to tweak, image.repository and image.tag, you can deploy a new application in no time. It's an ideal starting point for those taking their first steps in Kubernetes.</p> </li> <li> <p>Flexibility for Advanced Users: For the seasoned Kubernetes users, Kubedeploy offers a playground of possibilities. Define custom values to fine-tune and extend your deployments precisely as you need. It adapts to your expertise, ensuring your applications are deployed with the precision you demand.</p> </li> <li> <p>Compatibility Across Scenarios: Kubedeploy doesn't play favorites; it aims to be compatible with a wide range of deployment scenarios. Whether you're deploying microservices, web applications, or databases, Kubedeploy's got your back.</p> </li> </ul>"},{"location":"index.html#what-kubedeploy-isnt","title":"What Kubedeploy isn't","text":"<ul> <li> <p>Not a Replacement for Specialized Charts: While Kubedeploy is a powerful tool, it's not meant to replace specialized application charts tailored to specific software. It complements them by offering a more generalized approach to deployment.</p> </li> <li> <p>Not a Deployment Engine: Kubedeploy is a facilitator, not a replacement for essential deployment engines like Helm. You'll still need these tools to orchestrate and manage the deployment process.</p> </li> </ul>"},{"location":"examples/index.html","title":"Examples","text":"<p>In this section you can find various examples of using Kubedeploy with release management tols such as Helmfile.</p> <p>In hope it will make it easier to showcase it's capabilities, this section will also list various deployment configurations for some dockerized applications that don't provide their own Helm charts.</p> <p>All examples application deployments are provided on AS-IS without warranty.</p> <p>To get a full overview of Kubedeploy configuration, please see Reference page, and Value Documentation page.</p>"},{"location":"examples/helmfile.html","title":"Helmfile","text":"<p>Helmfile is a declarative spec for deploying Helm charts. It lets you\u2026</p> <ul> <li>Keep a directory of chart value files and maintain changes in version control.</li> <li>Apply CI/CD to configuration changes.</li> <li>Periodically sync to avoid skew in environments.</li> </ul> <p>To avoid upgrades for each iteration of Helm, the Helmfile executable delegates to Helm - as a result, Helm must be installed.</p> <p>This page will offer some usage examples of deplying Kubedeploy chart with Helmfile. It's by no means a replacement for Helmfile documentation, for full reference of configuration values and Helmfile options, please visit the official Helmfile page.</p>"},{"location":"examples/helmfile.html#quick-start","title":"Quick start","text":"<ol> <li> <p>Install Helmfile or use it from container</p> </li> <li> <p>Create helmfile.yaml configuration</p> </li> </ol> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\nreleases:\n- name: my-webserver\nnamespace: apps\nchart: sysbee/kubedeploy\ndisableValidationOnInstall: true\nversion: 1.1.0\ninstalled: true\nvalues:\n- image:\nrepository: nginx\ntag: latest\n</code></pre> <ol> <li>Deploy with helmfile</li> </ol> <pre><code>helmfile --file helmfile.yaml apply\n</code></pre>"},{"location":"examples/helmfile.html#deploying-multiple-releases","title":"Deploying multiple releases","text":"<p>From quickstart we might not get really great benefits while handling single deploymet. Where Helmfile shines is the managing multiple releases, and their dependencies, while helping you template repetitive values.</p> <p>Deploy multiple applications with dependencies</p> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\nreleases:\n- name: my-webserver\nnamespace: apps\nchart: sysbee/kubedeploy\nversion: 1.1.0\ninstalled: true\nvalues:\n- image:\nrepository: nginx\ntag: latest\n- name: my-app\nnamespace: apps\nchart: sysbee/kubedeploy\nversion: 1.1.0\ninstalled: true\nneeds:\n- my-webserver\nvalues:\n- image:\nrepository: my-app\ntag: latest\n</code></pre> <p>Deploy command</p> <pre><code>helmfile --file helmfile.yaml apply --skip-needs=false\n</code></pre> <p>When making modification to our release values we can also do preview of changes prior to applying them directly to the cluster</p> <p>Example</p> <pre><code>helmfile --file helmfile.yaml diff\n</code></pre> <p>Will output any changes in our releases</p>"},{"location":"examples/helmfile.html#templating-releases","title":"Templating releases","text":"<p>If we wish to follow the Best practices for multiple releases managed with Helmfile we can easily define all the best practices in release template, and reuse it in all our releases:</p> <p>Release templating</p> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\ntemplates:\ndefault: &amp;default\nnamespace: apps\nchart: sysbee/kubedeploy\nversion: 1.1.0\ninstalled: true\nvaluesTemplate:\n- nameOverride: '{{`{{ .Release.Name }}`}}'\n- replicaCount: 3\n- podDisruptionBudget:\nenabled: true\n- topologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: host\nwhenUnsatisfiable: ScheduleAnyway\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/instance: '{{`{{ .Release.Name }}`}}'\nreleases:\n- name: my-webserver\n&lt;&lt;: *default\nvalues:\n- replicaCount: 2\n- image:\nrepository: nginx\ntag: latest\n- name: my-app\n&lt;&lt;: *default\nneeds:\n- my-webserver\nvalues:\n- image:\nrepository: my-app\ntag: latest\n</code></pre> <p>All releases referencing the <code>default</code> anchor will inherit settings from default template. We can then easily keep common configuration values in one place, and override them per release if we need to.</p>"},{"location":"examples/helmfile.html#using-environment-variables","title":"Using environment variables","text":"<p>Helmfile can use environment variables as inputs for chart values which can be pretty useful in defining variable values via CI/CD pipelines.</p> <p>Users can define variables or secrets in CI/CD settings on GitHub or GitLab projects and then reference them in <code>helmfile.yaml</code></p> <p>Using environment variables</p> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\nreleases:\n- name: my-webserver\nnamespace: apps\nchart: sysbee/kubedeploy\ndisableValidationOnInstall: true\nversion: 1.1.0\ninstalled: true\nvalues:\n- image:\nrepository: nginx\ntag: latest\n- extraSecrets:\n- name: app-secret\ntype: Opaque\ndata:\nusername: {{ requiredEnv \"NGINX_USERNAME\" }}\npassword: {{ requiredENV \"NGINX_PASSWORD\" }}\n</code></pre> <p>Let's export the env vars before running helmfile</p> export env vars<pre><code>export NGINX_USERNAME=user\nexport NGINX_PASSWORD=supersecret\n</code></pre> <p>Deploy command</p> <pre><code>hemfile --file helmfile.yaml apply\n</code></pre>"},{"location":"examples/apps/joplin.html","title":"Joplin server","text":"<p>Joplin is an open source note-taking app. Capture your thoughts and securely access them from any device.</p> <p>Joplin support synchronization to various targets including open source Joplin server.</p> <p>Server application requires exteranal PostgreSQL datababase for storing notes. In this example we will use Kubedeploy to deploy Joplin server, and Bitnami PostgreSQL chart for deploying the database.</p>"},{"location":"examples/apps/joplin.html#environment-variables","title":"Environment variables","text":"<p>In this example we will use couple of environment variables in <code>helmfile.yaml</code></p> <ul> <li><code>JOPLIN_PG_PASS</code> - PostgreSQL password for joplin user</li> <li><code>JOPLIN_PG_POSTGRESPASS</code> - PostgreSQL password for postgres user</li> <li><code>JOPLIN_DOMAIN</code> - domain on which we will expose Joplin server</li> </ul>"},{"location":"examples/apps/joplin.html#deploying-with-kubedeploy","title":"Deploying with Kubedeploy","text":"<p>We will use Helmfile to define our release and configuration.</p> <p>Note</p> <p>Make sure required environment variables are configured either by exporting them on command line or by configuring them in CI/CD pipeline settings</p> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\n- name: bitnami-pg\nurl: registry-1.docker.io/bitnamicharts\noci: true\nreleases:\n- name: joplin-pg\nnamespace: apps\nchart: bitnami-pg/postgresql\ninstalled: true\nversion: 12.6.0\nvalues:\n- fullnameOverride: joplin-pg\n- auth:\nusername: joplin\ndatabase: joplin\npassword: {{ requiredEnv \"JOPLIN_PG_PASS\" }}\npostgresPassword: {{ requiredEnv \"JOPLIN_PG_POSTGRESPASS\" }}\n- metrics:\nenabled: true\nserviceMonitor:\nenabled: true\nlabels:\nrelease: prometheus\n- name: joplin-server\nnamespace: apps\nchart: sysbee/kubedeploy\nneeds:\n- apps/joplin-pg\ninstalled: true\nversion: 1.1.0\nvalues:\n- fullnameOverride: joplin-server\n- image:\nrepository: florider89/joplin-server\ntag: latest\npullPolicy: Always\n- ports:\n- name: http\ncontainerPort: 22300\nprotocol: TCP\n- healthcheck:\ndisableAutomatic: true\nenabled: true\nprobes:\nlivenessProbe:\nhttpGet:\npath: /api/ping\nport: 22300\nhttpHeaders:\n- name: Host\nvalue: {{ requiredEnv \"JOPLIN_DOMAIN\" }}\ninitialDelaySeconds: 3\nperiodSeconds: 5\nreadinessProbe:\nhttpGet:\npath: /api/ping\nport: 22300\nhttpHeaders:\n- name: Host\nvalue: {{ requiredEnv \"JOPLIN_DOMAIN\" }}\ninitialDelaySeconds: 3\nperiodSeconds: 5\n- env:\n- name: APP_PORT\nvalue: \"22300\"\n- name: APP_BASE_URL\nvalue: https://{{ requiredEnv \"JOPLIN_DOMAIN\" }}\n- name: DB_CLIENT\nvalue: pg\n- name: POSTGRES_PASSWORD\nvalue: {{ requiredEnv \"JOPLIN_PG_PASS\" }}\n- name: POSTGRES_DATABASE\nvalue: joplin\n- name: POSTGRES_USER\nvalue: joplin\n- name: POSTGRES_PORT\nvalue: \"5432\"\n- name: POSTGRES_HOST\nvalue: joplin-pg\n- ingress:\nenabled: true\nhosts:\n- host: {{ requiredEnv \"JOPLIN_DOMAIN\" }}\n</code></pre> <p>Deploy command</p> <pre><code>helmfile --file helmfile.yaml apply --skip-needs=false\n</code></pre>"},{"location":"examples/apps/karma.html","title":"Karma","text":"<p>Alertmanager UI is useful for browsing alerts and managing silences, but it's lacking as a dashboard tool - Karma aims to fill this gap.</p> <p>In this example we will deploy Karma and use oauth2-proxy container to facilitate SSO authentication to Karma's web interface.</p> <p>In this example we will use GitLab as oauth provider.</p>"},{"location":"examples/apps/karma.html#oidc-preparations","title":"OIDC preparations","text":"<p>Before continuing with application configuration, we need to create Oauth application in GitLab. Please follow the official documentation and store the clientID and Secret as environment variables.</p> <p>Required environment variables:</p> <ul> <li><code>OIDC_APPLICATION_ID</code> - obtained from GitLab</li> <li><code>OIDC_SECRET</code> - obtained from GitLab</li> <li><code>OIDC_COOKIE_SECRET</code> - generate using oauth2-proxy docs example</li> <li><code>KARMA_DOMAIN</code> - domain name on which we will run Karma</li> <li><code>GITLAB_GROUP</code> - members of this GitLab group will have access to Karma</li> </ul> <p>export the env variables from shell or save them as secrets/variables in your CI/CD pipeline configuration.</p>"},{"location":"examples/apps/karma.html#deploying-with-kubedeploy","title":"Deploying with Kubedeploy","text":"<p>We will use Helmfile to define our release and configuration.</p> <p>Let's now create a deployment configuration</p> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\nreleases:\n- name: karma\nnamespace: monitoring\nchart: sysbee/kubedeploy\nversion: 1.1.0\ninstalled: true\nvalues:\n- fullnameOverride: karma\n- image:\nrepository: ghcr.io/prymitive/karma\ntag: \"v0.115\"\n# Expose Karma container Port\n- ports:\n- name: http\ncontainerPort: 8080\nprotocol: TCP\n# Enable Karma healtchecks\n- healthcheck:\ndisableAutomatic: true\nenabled: true\nprobes:\nlivenessProbe:\nhttpGet:\npath: /health\nport: http\ninitialDelaySeconds: 5\nperiodSeconds: 5\nreadinessProbe:\nhttpGet:\npath: /health\nport: http\ninitialDelaySeconds: 5\nperiodSeconds: 5\n# Restrict the container privileges\n- securityContext:\nrunAsUser: 2000\nallowPrivilegeEscalation: false\n# Define required env variables\n- env:\n- name: CONFIG_FILE # (1)\nvalue: \"/etc/karma/karma.conf\"\n- name: OAUTH2_PROXY_CLIENT_ID\nvalue: {{ requiredEnv \"OIDC_APPLICATION_ID\" | quote }}\n- name: OAUTH2_PROXY_CLIENT_SECRET\nvalue: {{ requiredEnv \"OIDC_SECRET\" | quote }}\n- name: OAUTH2_PROXY_COOKIE_SECRET\nvalue: {{ requiredEnv \"OIDC_COOKIE_SECRET\" | quote }}\n- name: OAUTH2_PROXY_EMAIL_DOMAINS\nvalue: \"*\"\n- name: OAUTH2_PROXY_UPSTREAMS\nvalue: \"http://127.0.0.1:8080/\"\n- name: OAUTH2_PROXY_REDIRECT_URL\nvalue: \"https://{{ requiredEnv \"KARMA_DOMAIN\" }}/oauth2/callback\"\n- name: OAUTH2_PROXY_COOKIE_DOMAIN\nvalue: {{ requiredEnv \"KARMA_DOMAIN\" | quote }}\n- name: OAUTH2_PROXY_COOKIE_EXPIRE\nvalue: \"8h\"\n- name: OAUTH2_PROXY_COOKIE_SECURE\nvalue: \"true\"\n- name: OAUTH2_PROXY_PROVIDER\nvalue: \"gitlab\"\n- name: OAUTH2_PROXY_OIDC_ISSUER_URL\nvalue: \"https://gitlab.com\"\n- name: OAUTH2_PROXY_REVERSE_PROXY\nvalue: \"true\"\n# Configure Oauth2-porxy container\n- additionalContainers:\nenabled: true\ncontainers:\n- name: oauth2-proxy\nrepository: quay.io/oauth2-proxy/oauth2-proxy\ntag: v7.4.0\nargs:\n- --http-address=0.0.0.0:8081\n- --real-client-ip-header=X-Forwarded-For\n- --footer=-\n- --gitlab-group={{ requiredEnv \"GITLAB_GROUP\" |quote }}\n- --custom-sign-in-logo=-\n- --pass-user-headers=true\nports:\n# Expose Oauth2-proxy container port\n- containerPort: 8081\nname: authenticated\nprotocol: TCP\n- service:\nports:\n- port: 8081\ntargetPort: authenticated\nprotocol: TCP\nname: authenticated\n- port: 8080 # (2)\ntargetPort: http\nprotocol: TCP\nname: http\n- ingress:\nenabled: true\n# Use Oauth2-proxy port for routing external traffic\nsvcPort: 8081\nhosts:\n- host: {{ requiredEnv \"KARMA_DOMAIN\" }}\n# Define configMap that will be used as karma config file\n- configMaps:\n- name: config\nmount: True\nmountPath: /etc/karma\ndata:\nkarma.conf: |\nauthentication:\nheader:\nname: X-Forwarded-Email\nvalue_re: ^(.+)$\nalertmanager:\ninterval: 30s\nservers:\n- name: cluster\nuri: http://alertmanager-operated:9093\ntimeout: 10s\ncluster: cluster\nproxy: true\nhealthcheck:\nfilters:\nwatchdog:\n- alertname=Watchdog\nalertAcknowledgement:\nenabled: true\nduration: 15m0s\nauthor: karma\ncomment: ACK! This alert was acknowledged using karma on %NOW%\nannotations:\ndefault:\nhidden: true\nvisible:\n- value\n- summary\nhidden:\n- help\n- runbook\nkeep: []\nstrip: []\norder:\n- value\n- summary\n- description\nlabels:\ncolor:\nstatic: []\nunique:\n- group\n- path\ncustom:\n\"@cluster\":\n- value_re: \".*\"\ncolor: \"#3683CB\"\ninstance:\n- value_re: \".*\"\ncolor: \"#d4f081\"\nseverity:\n- value: critical\ncolor: \"#C0392B\"\n- value: scrit\ncolor: \"#AF7AC5\"\n- value: warning\ncolor: \"#F4D03F\"\n- value: unknown\ncolor: \"#D35400\"\n- value: info\ncolor: \"#85C1E9\"\n- value_re: \".*\"\ncolor: \"#99A3A4\"\nservice:\n- value_re: \".*\"\ncolor: \"#9B59B6\"\nvalue:\n- value_re: \".*\"\ncolor: \"#9B59B6\"\ncheck:\n- value_re: \".*\"\ncolor: \"#9B59B6\"\nkeep: []\nstrip:\nvalueOnly:\n- alertname\n- instance\nfilters:\ndefault:\n- \"@state=active\"\nkarma:\nname: \"Karma\"\ngrid:\nsorting:\norder: label\nreverse: false\nlabel: severity\ncustomValues:\nlabels:\nseverity:\ncritical: 1\nwarning: 2\nunknown: 3\ninfo: 4\nhistory:\nenabled: true\ntimeout: 20s\nworkers: 30\nui:\nrefresh: 30s\nhideFiltersWhenIdle: false\ncolorTitlebar: false\ntheme: \"dark\"\nanimations: true\nminimalGroupWidth: 420\nalertsPerGroup: 7\ncollapseGroups: collapsedOnMobile\nmultiGridLabel: \"\"\nmultiGridSortReverse: false\n</code></pre> <ol> <li>Karma config file is later added as <code>configMaps</code> mounted at this location.</li> <li>Karma port is not strictly required here if you don't have any other services in the cluster that needs to contact Karma without authentication.</li> </ol> <p>Deploy command</p> <pre><code>kubedeploy --file kubedeploy.yaml apply\n</code></pre> <p>Your protected Karma instance should now be available at <code>KARMA_DOMAIN</code></p>"},{"location":"examples/apps/mealie.html","title":"Mealie","text":"<p>Mealie is an intuitive and easy to use recipe management app. It's designed to make your life easier by being the best recipes management experience on the web and providing you with an easy to use interface to manage your growing collection of recipes.</p> <p>Mealie deployment is consisting of two containers, one for frontend and one for backend. It's database (sqlite) is stored on disk so we will be deploying this app as StatefulSet.</p>"},{"location":"examples/apps/mealie.html#deploying-with-kubedeploy","title":"Deploying with Kubedeploy","text":"<p>We will use Helmfile to define our release and configuration.</p> <p>Export the Mealie domain as environment variable</p> <pre><code>export MEALIE_DOMAIN=mealie.my-domain.com\n</code></pre> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\nreleases:\n- name: mealie\nnamespace: apps\nchart: sysbee/kubedeploy\ndisableValidationOnInstall: true\nversion: 1.1.0\ninstalled: true\nvalues:\n- deploymentMode: Statefulset\n- fullnameOverride: mealie\n- image:\nrepository: hkotel/mealie\ntag: frontend-nightly\npullPolicy: Always\n- ports:\n- name: http\ncontainerPort: 3000\nprotocol: TCP\n- service:\nenabled: true\nports:\n- port: 3000\ntargetPort: http\nprotocol: TCP\nname: http\n- ingress:\nenabled: true\nhosts:\n- host: {{ requiredEnv \"MEALIE_DOMAIN\" }}\n- env:\n- name: API_URL\nvalue: http://localhost:9000\n- name: BASE_URL\nvalue: https://{{ requiredEnv \"MEALIE_DOMAIN\" }}\n- name: ALLOW_SIGNUP\nvalue: \"false\"\n- name: DB_ENGINE\nvalue: sqlite\n- name: WEB_GUNICORN\nvalue: \"false\"\n- name: WORKERS_PER_CORE\nvalue: \"0.5\"\n- name: MAX_WORKERS\nvalue: \"1\"\n- name: WEB_CONCURRENCY\nvalue: \"1\"\n- resources:\nlimits:\nmemory: 512Mi\n- additionalContainers:\nenabled: true\ncontainers:\n- name: mealie-api\nrepository: hkotel/mealie\ntag: \"api-nightly\"\nports:\n- name: api\ncontainerPort: 9000\nprotocol: TCP\nresources:\nlimits:\nmemory: 1Gi\nhealthcheck:\nenabled: false\nprobes:\nlivenessProbe:\nhttpGet:\npath: /api/app/about\nport: 9000\ninitialDelaySeconds: 3\nperiodSeconds: 3\nreadinessProbe:\nhttpGet:\npath: /api/app/about\nport: 9000\ninitialDelaySeconds: 3\nperiodSeconds: 3\n- persistency:\nenabled: true\ncapacity:\nstorage: 20Gi\nmountPath: \"/app/data\"\n</code></pre> <p>Deploy command</p> <pre><code>helmfile --file helmfile.yaml apply\n</code></pre> <p>Open your browser at <code>MEALIE_DOMAIN</code></p>"},{"location":"examples/apps/syncthing.html","title":"Syncthing","text":"<p>Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it\u2019s transmitted over the internet.</p>"},{"location":"examples/apps/syncthing.html#deploying-with-kubedeploy","title":"Deploying with Kubedeploy","text":"<p>We will use Helmfile to define our release and configuration.</p> helmfile.yaml<pre><code>---\nrepositories:\n- name: sysbee\nurl: https://charts.sysbee.io/stable/sysbee\nreleases:\n- name: syncthing\nnamespace: apps\nchart: sysbee/kubedeploy\ndisableValidationOnInstall: true\nversion: 1.1.0\ninstalled: true\nvalues:\n# one replica should be enough\n- replicaCount: 1\n# We wish to preserve Synthing data over restarts\n- deploymentMode: Statefulset\n- image:\nrepository: syncthing/syncthing\ntag: latest\npullPolicy: Always\n- ports:\n# Admin UI pot\n- name: http\ncontainerPort: 8384\nprotocol: TCP\n# Sync ports\n- name: syncthingtcp\ncontainerPort: 22000\nprotocol: TCP\n- name: syncthingudp\ncontainerPort: 22000\nprotocol: UDP\n# Enable healtchecks for automatic container restart if it stops responding\n- healthcheck:\nenabled: true\nprobes:\nlivenessProbe:\ntcpSocket:\nport: http\nreadinessProbe:\ntcpSocket:\nport: http\n# Define service as NodePort, each Syncthing pod will need direct communication\n# with other peers. We explicitly skip opening Admin UI port via service\n- service:\nenabled: true\ntype: NodePort\nports:\n- port: 22000\ntargetPort: syncthingtcp\nprotocol: TCP\nname: syncthingtcp\n- port: 22000\ntargetPort: syncthingudp\nprotocol: UDP\nname: syncthingudp\n# Configure persistent volume for Syncthing data\n- persistency:\n# enable persistent volume\nenabled: true\ncapacity:\nstorage: 10Gi\nmountPath: \"/var/syncthing\"\n</code></pre> <p>Deploy command</p> <pre><code>helmfile --file helmfile.yaml apply\n</code></pre>"},{"location":"examples/apps/syncthing.html#connect-to-admin-ui","title":"Connect to Admin UI","text":"<p>Start the port forwarding to Admin UI port <pre><code>kubectl -n apps port-forward syncthing-kubdeploy-0 8384:8384\n</code></pre></p> <p>Open your browser at</p> <p>http://localhost:8384</p>"},{"location":"reference/values.html","title":"kubedeploy","text":"<p>Homepage: https://kubedeploy.app/</p>"},{"location":"reference/values.html#maintainers","title":"Maintainers","text":"Name Email Url Branko Toic branko@sysbee.net https://www.sysbee.net"},{"location":"reference/values.html#installing-the-chart","title":"Installing the Chart","text":"<p>To install the chart with the release name <code>my-release</code>:</p> <pre><code>$ helm repo add sysbee https://charts.sysbee.io/stable/sysbee\n$ helm install my-release sysbee/kubedeploy\n</code></pre>"},{"location":"reference/values.html#requirements","title":"Requirements","text":"<p>Kubernetes: <code>&gt;=1.20.0-0</code></p>"},{"location":"reference/values.html#values","title":"Values","text":"Key Type Default Description   additionalContainers object see subvalues additionalContainers settings  additionalContainers.containers list see subvalues Sequential list of additionalContainers.  additionalContainers.containers[0].args optional <code>[]</code> Define custom arguments for additionalContainer  additionalContainers.containers[0].command optional <code>[\"sh\",\"-c\",\"while true; do sleep 30; done;\"]</code> Define custom command for additionalContainer  additionalContainers.containers[0].healthcheck optional see subvalues Define healthcheck probes for additionalContainer  additionalContainers.containers[0].healthcheck.enabled bool <code>false</code> Enable custom healthcheck probes for additionalContainer  additionalContainers.containers[0].healthcheck.probes.livenessProbe object <code>{}</code> Define livenessProbe  additionalContainers.containers[0].healthcheck.probes.readinessProbe object <code>{}</code> Define readinessProbe  additionalContainers.containers[0].healthcheck.probes.startupProbe object <code>{}</code> Define startupProbe  additionalContainers.containers[0].ports optional <code>[]</code> Define additionalContainer exposed ports see: containerPort  additionalContainers.containers[0].repository required <code>\"busybox\"</code> Define additionalContainer repository  additionalContainers.containers[0].resources optional <code>{}</code> Define custom resources for this specific additionalContainer.  additionalContainers.containers[0].tag optional <code>\"latest\"</code> Define additionalContainer image tag  additionalContainers.enabled bool <code>false</code> Define if additionalContainers are enabled  additionalContainers.pullPolicy optional <code>\"IfNotPresent\"</code> additionalContainers image pull policy  additionalContainers.resources optional <code>{}</code> Define additionalContainers global resource requests and limits. Will be applied to all additionalContainers if more specific (per container) resource requests and limits are not defined   affinity object <code>{}</code> Define Pod affinity disables automatic podAntiAffinity rules if defined.   autoscaling object see subvalues Autoscaling settings  autoscaling.behavior object <code>{}</code> HPA configurable scaling behavior see ref  autoscaling.enabled bool <code>false</code> Enable autoscaling. Works only with deploymentMode=Deployment  autoscaling.maxReplicas int <code>10</code> Maximum number of Pod replicas  autoscaling.minReplicas int <code>1</code> Minimum number of Pod replicas  autoscaling.targetCPUUtilizationPercentage int <code>80</code> Scaling target CPU utilization as percentage of resources.requests.cpu  autoscaling.targetMemoryUtilizationPercentage int <code>nil</code> Scaling target memory utilization as percentage of resources.requests.mem   configMaps list <code>[]</code> Define a list of extra ConfigMap objects. See values.yaml or chart documentation for examples on syntax   configMapsHash bool <code>false</code> Redeploy Deployments and Statefulsets if deployed ConfigMaps content change.   cronjobspec object see subvalues Cronjobspec settings  cronjobspec.args list <code>[]</code> Define args for cronjob  Starting from Kubedeploy version 1.2 you should start using <code>image.args</code> instead of <code>cronjobspec.args</code>. Values will be available as failsafe up to Kubedeploy 2.0 when they will be removed.  cronjobspec.backoffLimit int <code>3</code> Define job backoff limit, see reference  cronjobspec.command list <code>[]</code> Define command for cronjob  Starting from Kubedeploy version 1.2 you should start using <code>image.command</code> instead of <code>cronjobspec.command</code>. Values will be available as failsafe up to Kubedeploy 2.0 when they will be removed.  cronjobspec.concurrencyPolicy string <code>\"\"</code> Define concurrency policy options: Allow (default), Forbid or Replace, see reference  cronjobspec.failedJobsHistoryLimit int <code>1</code> Define number of failed Job logs to keep  cronjobspec.schedule string <code>\"0 * * * *\"</code> Define cronjob schedule, for details see reference  cronjobspec.startingDeadlineSeconds optional <code>180</code> Define deadline for starting the job, see reference  cronjobspec.successfulJobsHistoryLimit int <code>3</code> Define number of successful Job logs to keep   deploymentMode string <code>\"Deployment\"</code> Available deployment modes, currently supported:   <ul><li>Deployment</li> <li>Statefulset</li> <li>Job</li> <li>Cronjob</li> <li>None</li></ul>   env list <code>[]</code> Define environment variables for all containers in Pod. For reference see: env.   envFrom list <code>[]</code> Define environment variables from ConfigMap or Secret objects for all containers in Pod. For reference see envFrom secret example or envFrom configmap example   extraIngress list <code>[]</code> list of extra Ingress objects to deploy. extraIngress requires additional name: parametar. see ingress values for required spec or values example.   extraObjects list <code>[]</code> Create dynamic manifest via values (templated). See values.yaml or chart documentation for examples:   extraSecrets list <code>[]</code> Define a list of extra Secrets objects. See values.yaml or chart documentation for examples on syntax   extraVolumeMounts list <code>[]</code> Define extra volume mounts for containers See values.yaml or chart documentation for examples on syntax   fullnameOverride string <code>\"\"</code> Override full resource names instead of using calculated \"releasename-chartname\" default naming convention   healthcheck object see subvalues Healthcheck settings  healthcheck.disableAutomatic bool <code>false</code> Disable automatic healthcheck probes. Automatic probes will always create a HTTP healthcheck probe if container has port named http,  healthcheck.enabled bool <code>false</code> Define custom healthcheck probes for container. Overrides automatic probes.  healthcheck.probes.livenessProbe object <code>{}</code> Define livenessProbe  healthcheck.probes.readinessProbe object <code>{}</code> Define readinessProbe  healthcheck.probes.startupProbe object <code>{}</code> Define startupProbe   image object see subvalues Container image settings  image.args list <code>[]</code> Define container custom arguments. Reference  image.command list <code>[]</code> Define container custom command. Reference  image.lifecycle object <code>{}</code> Define container custom lifecycle hooks. More info  image.pullPolicy string <code>\"IfNotPresent\"</code> Default container pull policy  image.repository string <code>\"nginx\"</code> Define container repository  image.tag string <code>\"latest\"</code> Define container image tag  image.terminationGracePeriodSeconds int <code>30</code> Define Pod terminationGracePeriodSeconds. Should be greater then expected run time of lifecycle hooks   imagePullSecrets list <code>[]</code> Define ImagePullSecrets   ingress object see subvalues Ingress object settings.  ingress.annotations object <code>{\"cert-manager.io/cluster-issuer\":\"letsencrypt\"}</code> Additional Ingress annotations  ingress.className string <code>\"haproxy\"</code> Ingress class name  ingress.enabled bool <code>false</code> Enable Ingres for release  ingress.hosts list see subvalues Ingress host list.  ingress.hosts[0].host string, required <code>\"\"</code> Define Ingress hostname  ingress.hosts[0].paths list <code>[]</code> Ingress host paths see values.yaml or chart documentation for examples  ingress.pathType string <code>\"ImplementationSpecific\"</code> Default Ingress pathType  ingress.svcPort string first port from service.ports Define default Service port that will be targeted by Ingress.  ingress.tls list <code>[]</code> Ingress TLS list. overrides any auto configured tls config created by withSSL.  Allows for custom secretName and host list to be defined. For cases where you have pre-configured SSL stored as Kubernetes secret. If secret does not exist, new one will be created by cert-manager. see values.yaml or chart documentation for examples:  ingress.withSSL bool <code>true</code> Deploy Ingress object with SSL support. Automatically configures the Ingress tls spec with all the configured ingress.hosts in one Secret.   initContainers object see subvalues initContainers settings  initContainers.containers list see subvalues Sequential list of initContainers.  initContainers.containers[0].args optional <code>[]</code> Define custom arguments for initContainer  initContainers.containers[0].command optional <code>[\"sh\",\"-c\",\"exit 0\"]</code> Define custom command for initContainer  initContainers.containers[0].name required busybox-init Define initContainer name  initContainers.containers[0].repository required <code>\"busybox\"</code> Define initContainer repository  initContainers.containers[0].resources optional <code>{}</code> Define custom resources for this specific initContainer.  initContainers.containers[0].tag optional <code>\"latest\"</code> Define initContainer image tag  initContainers.enabled bool <code>false</code> Define if initContainers are enabled.  initContainers.pullPolicy optional <code>\"IfNotPresent\"</code> initContainers image pull policy  initContainers.resources optional <code>{}</code> Define initContainers global resource requests and limits. Will be applied to all initContainers if more specific (per container) resource requests and limits are not defined   jobspec object see subvalues jobspec settings  jobspec.args list <code>[]</code> Define args for Job  Starting from Kubedeploy version 1.2 you should start using <code>image.args</code> instead of <code>jobspec.args</code>. Values will be available as failsafe up to Kubedeploy 2.0 when they will be removed.  jobspec.backoffLimit int <code>3</code> Define Job backoff limit, see reference  jobspec.command list <code>[]</code> Define command for Job  Starting from Kubedeploy version 1.2 you should start using <code>image.command</code> instead of <code>jobspec.command</code>. Values will be available as failsafe up to Kubedeploy 2.0 when they will be removed.  jobspec.parallelism int <code>1</code> Define Job paralelisam, see reference  jobspec.restartPolicy string <code>\"OnFailure\"</code> Define restart policy for jobs if deploymentMode=Job, see reference  jobspec.ttlSecondsAfterFinished string <code>\"300\"</code> Define Automatic Cleanup for Finished Jobs   keda object see subvalues Keda settings  keda.behavior object <code>{}</code> HPA configurable scaling behavior see ref  keda.cooldownPeriod int <code>300</code> The period to wait after the last trigger reported active before scaling the resource back to 0 ref  keda.enabled bool <code>false</code> Kubernetes Event-driven Autoscaling: KEDA 2.x ref Note: mutually exclusive with HPA, enabling KEDA disables HPA  keda.maxReplicas int <code>10</code> Number of maximum replicas for KEDA autoscaling  keda.minReplicas int <code>1</code> Number of minimum replicas for KEDA autoscaling  keda.pollingInterval int <code>30</code> Interval for checking each trigger ref  keda.restoreToOriginalReplicaCount bool <code>false</code> After scaled object is deleted return workload to initial replica count ref  keda.scaledObject.annotations object <code>{}</code> Scaled object annotations, can be used to pause scaling ref  keda.triggers list <code>[]</code> Keda triggers ref see values for prometheus example   kubeVersionOverride string <code>\"\"</code> Allow override of auto-detected Kubernetes version   minReadySeconds int <code>10</code> Define minReadySeconds for deployments and statefulsets   monitoring object see subvalues Monitoring settings. Will define Prometheus ServiceMonitor or PodMonitor objects.  monitoring.enabled bool <code>false</code> Enable monitoring. If service.enabled=True chart will generate ServiceMonitor object, otherwise PodMonitor will be used.  monitoring.labels object <code>{}</code> Provide additional labels to the ServiceMonitor metadata  monitoring.metricRelabelings list <code>[]</code> Provide additional metricRelabelings to apply to samples before ingestion.  monitoring.relabelings list <code>[]</code> Provide additional relabelings to apply to samples before scraping  monitoring.scheme string <code>\"http\"</code> HTTP scheme to use for scraping.  monitoring.scrapeInterval string <code>\"20s\"</code> Provide interval at which metrics should be scraped  monitoring.scrapePath string <code>\"/metrics\"</code> Provide HTTP path to scrape for metrics.  monitoring.scrapePort string <code>\"metrics\"</code> Provide named service port used for scraping  monitoring.scrapeTimeout string <code>\"15s\"</code> Timeout after which the scrape is ended (must be less than scrapeInterval)  monitoring.targetLabels list <code>[]</code> Additional metric labels  monitoring.tlsConfig object <code>{}</code> TLS configuration to use when scraping the endpoint   nameOverride string <code>\"\"</code> Override release name used in calculated \"releasename-chartname\" default naming convention   networkPolicy object see subvalues networkPolicy settings  networkPolicy.egress list <code>[]</code> Define spec.egress for NetowkPolicy rules  networkPolicy.enabled bool <code>false</code> Enables Pod based NetworkPolicy  networkPolicy.ingress list <code>[]</code> Define spec.ingress for NetowkPolicy rules  networkPolicy.ingressNamespace string <code>\"ingress\"</code> Define Namespace where Ingress controller is deployed. Used to generate automatic policy to enable ingress access when .Values.ingress is enabled  networkPolicy.monitoringNamespace string <code>\"monitoring\"</code> Define namespace where monitoring stack is deployed Used to generate automatic policy to enable monitoring access when .Values.monitoring is enabled   nodeSelector object <code>{}</code> Define custom node selectors for Pod   persistency object see subvalues persistency settings  persistency.accessModes list <code>[\"ReadWriteOnce\"]</code> Define storage access modes. Must be supported by available storageClass  persistency.capacity.storage string <code>\"5Gi\"</code> Define storage capacity  persistency.enabled bool <code>false</code> Enable support for persistent volumes. Supported only if deploymentMode=Statefulset.  persistency.mountPath string <code>\"/data\"</code> Define where persistent volume will be mounted in containers.  persistency.storageClassName string uses cluster default storageClassName Define custom name for persistent storage class name   podAnnotations object <code>{}</code> Define Pod annotations   podAntiAffinity string <code>\"\"</code> Pod anti-affinity can prevent the scheduler from placing application replicas on the same node. The default value \"soft\" means that the scheduler should prefer to not schedule two replica pods  onto the same node but no guarantee is provided. The value \"hard\" means that the scheduler is required to not schedule two replica pods onto the same node. The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.   podAntiAffinityTopologyKey string <code>\"kubernetes.io/hostname\"</code> If anti-affinity is enabled sets the topologyKey to use for anti-affinity. This can be changed to, for example, failure-domain.beta.kubernetes.io/zone   podDisruptionBudget object see subvalues podDisruptionBudget settings  podDisruptionBudget.enabled bool <code>false</code> Enable Pod disruption budget see: podDisruptionBudget  podDisruptionBudget.maxUnavailable int <code>nil</code> Maximum unavailable replicas  podDisruptionBudget.minAvailable int <code>1</code> Minimum available replicas   podExtraLabels object <code>{}</code> Define Pod extra labels   podSecurityContext object <code>{}</code> Define Pod securityContext   ports list <code>[]</code> Define container exposed ports see: containerPort   replicaCount int <code>1</code> Number of Pod replicas to be deployed. Applicable to Deployment and Statefulset deploymentMode   resources object <code>{}</code> Container resource requests and limits. Resource requests and limits are left as a conscious choice for the user. This also increases chances charts run on environments with little resources, such as Minikube. See resources for syntax   securityContext object <code>{}</code> Define container securityContext   service object see subvalues Service object settings  service.enabled bool <code>true</code> Enable Service provisioning for release.  service.headless bool <code>false</code> Create a headless service. See reference  service.ports list <code>[]</code> Define listening ports for Service. If unspecified, chart will automatically generate ports list based on main container exposed ports. see values.yaml or chart documentation for syntax examples  service.type string <code>\"ClusterIP\"</code> Set Service type. See Service for available options.   serviceAccount object see subvalues serviceAccount settings  serviceAccount.annotations object <code>{}</code> Annotations to add to the service account  serviceAccount.create bool <code>true</code> Specifies whether a service account should be created  serviceAccount.name string <code>\"\"</code> The name of the service account to use. If not set and create is true, a name is generated using the fullname template   tolerations list <code>[]</code> Define Pod tolerations   topologySpreadConstraints list <code>[]</code> Define custom topologySpreadConstraings for Pod <p>Autogenerated from chart metadata using helm-docs v1.12.0</p>"},{"location":"start/index.html","title":"Examples by Values","text":"<p>This section lists all the Kubedeploy's top level configurable values, as well as their subvalues with examples on how to use them.</p> <p>Where applicable, documentation page will list the defaults, and explain behind the scenes  to gain better understanding what will happen when you modify those values.</p> <p>All the values are documented in chart's default <code>values.yaml</code> file as well which can be good starting point for exploring.</p> <p>Retrieving default values.yaml file</p> <pre><code>helm show values sysbee/kubedeploy\n</code></pre> <p>If you would like to experiment with the configurable values and see their output on rendered manifests you can use Helm's template ability</p> <p>Previewing changes</p> <pre><code>helm template my-release sysbee/kubedeploy -f values.yaml\n</code></pre> <p>To get an insights into all the values with configurable subvalues please visit the Reference page.</p>"},{"location":"start/best-practices.html","title":"Best practices","text":"<p>When deploying applications to Kubernetes clusters, consider the following best practices when defining custom Kubedeploy values.</p> <p>In the following sections, we will cover some of the deployment configuration in the context of Kubedeploy configuration values. For the sake of simplicity, all of the examples will be provided as values in custom <code>value.yaml</code> file which can then be passed to Helm or other Helm chart release management software.</p> <p>Tip</p> <p>While it might be acceptable to disregard these best practices in a development environment, production environment should adhere to them. This adherence helps minimize potential application downtime, reduce errors during new application version rollouts, and enhance the application's resilience against resource shortages, among other benefits.</p>"},{"location":"start/best-practices.html#replicas","title":"Replicas","text":"<p>Application replicas play a significant role in ensuring application availability during new version rollouts, underlying infrastructure errors, and other configuration values that we will discuss later on this page.</p> <p>The Default replica count in Kubedeploy is <code>1</code>, meaning only one Pod of your application will run in cluster. As mentioned, this might be suitable for a development environment where the application is under no specific Service Level Objective (SLO) needs to be met.</p> <p>In the event of a Kubernetes node failure, the application will become unavailable until it is rescheduled on another node. During a new version rollout, a new single pod will be created with the updated version, and the old pod will then be terminated.</p> <p>Increasing the <code>replicaCount</code> value in Kubedeploy will create corresponding number of Pod replicas for your application in the cluster, enhancing the application's resiliency and throughput, if other complimentary configurations are properly set.</p> <p>replicaCount != High Availability</p> <p>However, it's important to note that increasing the replicaCount does not automatically guarantee high availability. The Kubernets scheduler might place all the Pod replicas on single node if Pod <code>AntiAffinity</code> is not configured, resulting in downtime if that specific Kubernetes node goes offline.</p> <p>On the downside, a higher replica count will consume more resources within the cluster, leading to increased operating costs. Configuring proper replicaCount is a balancing game between availability and infrastructure cost.</p> <p>increasing replicaCount</p> custom-values.yaml<pre><code>replicaCount: 3\n</code></pre> <p>To ensure greater application availability, it's recommended to have at least 2-3 replicas of the application running at all times.</p>"},{"location":"start/best-practices.html#assigning-pod-to-nodes","title":"Assigning Pod to nodes","text":"<p>By default, the Kubernetes scheduler aims to pack Pods as densely as possible, minimizing resource overhead on nodes. Consequently, the default scheduler always attempts to use the fewest number of nodes in the cluster.</p> <p>There are couple of ways to influence default scheduler and Pod placement strategy. In this section we will discuss affinity,  anti-affinity, as well as Pod topology spread constraints. For more detailed information, please follow the links to the official documentation.</p>"},{"location":"start/best-practices.html#affinity","title":"Affinity","text":"<p>The affinity configuration allows as to define soft (preferred) or hard (required) Pod affinity and anti-affinity rules for Pods.</p> <p>Kubedeploy allows for automatic setup of simple Pod anti-affinity rules:</p> <p>Simple Pod anti-affinity</p> Soft anti-affinity<pre><code>fullnameOverride: my-app\nreplicaCount: 3\npodAntiAffinity: \"soft\"\n</code></pre> <p>The above values will establish soft pod-anti-affinity rules based on Pod selector labels. In this scenario, Pods will strive to be placed on different hosts if resources are available. If there aren't enough nodes available in the cluster, but the Pod's resource requirement allows it, the scheduler might place two Pods on one node and the third Pod on another node. To ensure that the scheduler provisions Pods on different nodes, we can adjust the configuration as follows:</p> Hard anti-affinity<pre><code>fullnameOverride: my-app\nreplicaCount: 3\npodAntiAffinity: \"hard\"\n</code></pre> <p>In the previous example, the scheduler is required to place all Pod replicas on different nodes.</p> <p>With configured cluster autoscaler</p> <p>If there aren't enough available nodes (even if there are sufficient resources), the cluster autoscaler will be activated to provide additional nodes, meeting the hard anti-affinity requirement. If the autoscaler is unable to allocate more nodes, it will schedule as many Pod replicas as possible based on the requirement, with remaining Pods remaining in a Pending state until the requirement can be fulfilled.</p> <p>The simple Pod anti-affinity configuration permits the selection of the topology key for spreading the pods. Topology keys are labels assigned to Kubernetes nodes. Anti-affinity rules will search for values in those keys and ensure that each Pod is placed on a node with a different value for the given topology key.</p> <p>The default topology key for simple Pod anti-affinity rules is  <code>kubernetes.io/hostname</code>. However, this is configurable and can be changed based on use case.</p> <p>Simple Pod anti-affinity with zone key</p> Soft anti-affinity by zone<pre><code>fullnameOverride: my-app\nreplicaCount: 3\npodAntiAffinity: \"soft\"\npodAntiAffinityTopologyKey: kubernetes.io/zone\n</code></pre> <p>In the example above, Pods will be spread across multiple availability zones, enhancing application resilience to failures within a single AZ. Soft anti-affinity is preferred here, as the availability zone count is finite and oftentimes can't be increased. If the cluster is configured with only two availability zones, two Pod replicas would be placed in zone <code>A</code> and one Pod in zone <code>B</code>.</p> <p>Warning</p> <p>In case of hard anti-afinity rules based on zones, if the replicaCount is greater than the number of configured zones, some Pods will never be scheduled.</p> <p>Kubedeploy also provides the ability to define custom affinity and anti-affinity rules. This offers finer control over Pod placement, not only in relation to each other but also in relation to other deployments. Custom affinity rules can be created via <code>affinity</code> option in Kubedeploy values, which will be directly translated into Pods <code>spec.affinity</code>.</p> <p>Warning</p> <p>Defining custom affinity rules will disable any configured simple anti-affinity rules.</p> <p>Configuration of custom affinity rules is beyond the scope of this document; please refer to official documentation for usage examples.</p>"},{"location":"start/best-practices.html#topology-spread-constraints","title":"Topology Spread Constraints","text":"<p>For more detailed explanation on Pod Toplology Spread Constraints please refer to the official documentation.</p> <p>In essence, these constraints provide a flexible alternative to Pod Affinity/Anti-Affinity. Topology spread constraints let you separate nodes into groups and assign Pods using a label selector. They also allow you to instruct the scheduler on how (un)evenly to distribute those Pods.</p> <p>Topology spread constraints can overlap with other scheduling policies like Node Selector or taints. The last two fields of a Pod topology spread let you decide on the nature of these relations:</p> <p>nodeAffinityPolicy lets you decide how to treat your Pod\u2019s nodeAffinity and nodeSelector when calculating the topology spread skew. You get two options: 1. Honor only includes nodes matching nodeAffinity and nodeSelector. 2. Ignore, ignoring these settings and including all nodes in the calculations.</p> <p>The Honor approach is the default if you leave this field empty.</p> <p>nodeTaintsPolicy indicates how you wish to treat node taints when calculating Pod topology spread skew. Here you also get two options: Honor or Ignore, with the latter being followed if you leave this field empty.</p> <p>Kubedeploy offers the <code>toplologySpreadConstraints</code> configuration value to pass raw configuration. For more information on the format, please refer to the official examples.</p> <p>Spreading multiple Pod replicas evenly across availability zones</p> <p>Topology constraints offer similar \"soft\" and \"hard\" requiremnets in the form of <code>whenUnsatisfiable</code> option. The \"hard\" requierment is the default. Unlike the example with \"hard\" anti-affinity rules when selecting zone as toplology key, toplology constraint can successfully schedule more replicas (for example, 5) across 2 availability zones.</p> Hard toplologySpreadConstraint by zone<pre><code>fullnameOverride: my-app\nreplicaCount: 5\ntoplologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: kubernetes.io/zone\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name=my-app\n</code></pre> <p>In the example above, with cluster deployed in only two availability zones, three Pods would be deployed in zone <code>A</code> and two Pods in zone <code>B</code>.</p>"},{"location":"start/best-practices.html#node-selectors","title":"Node Selectors","text":"<p>Each node in the cluster is assigned a predefined set of common labels. Node labels can be utilized in Pod scheduling decisions by defining node selectors.</p> <p>Kubedeploy offers this functionality via the <code>nodeSelector</code> value config option, enabling you to specifically target a particular node group.</p> <p>Targeting specific node group</p> Node selectors<pre><code>fullnameOverride: my-app\nreplicaCount: 2\nnodeSelector:\nkubernetes.io/arch: arm64\n</code></pre> <p>In the example above, Pods will target nodes of arm64 architecture and will not attempt to schedule on nodes with the amd64 architecture.</p> <p>Warning</p> <p>Containers must be built specifically for arm64 or for both architectures to utilize the above-mentioned nodeSelector. arm64 nodes offer the same resource capacity as their amd64 counterparts usually at a considerably lower price.</p> <p>Pods can also target multiple known node selector labels to gain more fine-grained control over their placement. EKS clusters that utilize Karpenter can dynamically provision nodes based on requirements. If the provisioner is set to deploy specific instance types, Pods can use any of the well-known Node labels. Furthermore, Karpenter will label nodes based on their instance types, allowing the <code>nodeSelectors</code> to provision specific instance generation or family.</p> <p>Provisioning on specific instance family</p> Node selectors<pre><code>fullnameOverride: my-app\nreplicaCount: 2\nnodeSelector:\nkarpenter.k8s.aws/instance-family: m6g\n</code></pre> <p>From this example we will be provisioning Pods on <code>m6g</code> instance family. <code>kubernetes.io/arch</code> is implied to be arm64 as the m6g instances are only available for arm64.</p> <p>Provisioning on instance-category and generation</p> Node selectors<pre><code>fullnameOverride: my-app\nreplicaCount: 2\nnodeSelector:\nkarpenter.k8s.aws/instance-category: m\nkarpenter.k8s.aws/instance-generation: 6\n</code></pre> <p>In this example, we are targeting any instance type starting with <code>m6</code>, including:</p> <ul> <li>m6a</li> <li>m6g</li> <li>m6gd</li> <li>m6i</li> <li>m6id</li> <li>etc.</li> </ul> <p>Note</p> <p>If we specifically want to limit the scheduler to provision on arm64 instances, this should be explicitly defined as additional label in node selector: <code>kubernetes.io/arch: arm64</code>. This will restrict the scheduler to just <code>m6g</code> and <code>m6gd</code> instance family</p>"},{"location":"start/best-practices.html#node-taints-and-tolerations","title":"Node Taints and Tolerations","text":"<p>Nodes can be grouped for provisioning, isolating workloads from one another. For example, Kubernetes cluster might have a dedicated node group solely for database services. To achieve this, nodes can be configured with specific node taints. Pods that lack specified tolerations for those taints will avoid scheduling on such node groups.</p> <p>For more information on toleration and taints, please follow the official documentation.</p> <p>Kubedeploy offers the ability to define custom tolerations for Pods with <code>tolerations</code> value configuration.</p> <p>Provision node on specific node group with taints</p> <p>Let's assume we have a specific node group running the <code>c6</code> instance family, intended only for database workloads. Nodes have a specific label set: <code>workload-type: database</code>. Additionally, there's a taint  named <code>workload-type</code> with the value <code>database</code> and the effect <code>NoSchedule</code>. If we wish to target this specific node group, we can't rely solely on Karpenter's known labels. Using those labels would provision new nodes of the desired instance family, where other workloads could be scheduled if free capacity is available.</p> <p>To target this specific group, we need to use a <code>nodeSelector</code> with <code>tolerations</code>.</p> Node selectors and tolerations<pre><code>fullnameOverride: my-app\nreplicaCount: 2\nnodeSelector:\nworkload-type: database\ntolerations:\n- key: workload-type\nvalue: database\neffect: NoSchedule\n</code></pre> <p>This configuration targets the specific <code>workload-type: database</code> node group. Kubernets cluster administrators can then control which instance family is required for this workload on a system level, rather than defining the desired instance family in each deployment. The toleration key ensures that our Pods will tolerate the taints set on the node group.</p>"},{"location":"start/best-practices.html#spot-vs-on-demand-capacity-types","title":"Spot vs On-demand capacity types","text":"<p>Cloud provider specific</p> <p>This section covers AWS specific configuration, depending on your cloud provider you might need to adjust the settings accordingly.</p> <p>AWS offers two different capacity types for all their instance families:</p> <ul> <li>On-Demand: These instances are permanent, running on-demand workloads. They are non-interruptible capacity types, remaining operational until you decide to terminate them.</li> <li>Spot: This capacity type involves temporary, interruptible compute instances. They are sold by AWS when there is excess spare capacity in the region/zone. While identical in terms of compute power, spot instances offer a price saving of 50-70% compared to on-demand instances. However, spot instances can be terminated by AWS at any time in favor of on-demand instances, potentially causing workload interruption.</li> </ul> <p>Note</p> <p>Kubernetes workloads, at their core, should tolerate such interruptions well, especially if multiple application replicas are defined and spread across nodes, as explained earlier in this document.</p> <p>Handling spot instance interruptions</p> <p>When dealing with spot instance interruptions it is advisable to configure either Karpenter's built-in \"Interruption handling\" or configuring dedicated Node termination handler</p> <p>Karpenter provides the functionality to intercept spot instance interruption events. AWS sends a 2-minute termination notice before an instance is terminated. Karpenter will then:</p> <ol> <li>Flag the node with a special taint to prevent new scheduling activity.</li> <li>Provision a replacement node, considering workload node selection and resource requirements on the to-be-interrupted node.</li> <li>Pre-allocate workloads to the newly provisioned node.</li> <li>Initiate a graceful eviction of the node's workload, allowing Pods to finish their current operations and migrate to the new node.</li> <li>After eviction, either Karpenter decommissions the node scheduled for termination or AWS's Termination handler cleans up the node, whichever comes first.</li> </ol> <p>Warning</p> <p>This process is not the same as \"live migrating\" the Pod. Eviction and migration will interrupt Pods execution. Please make sure to read other best-practices on how to deal with this gracefully.</p> <p>When should I avoid spot instances?</p> <ul> <li>For non-HA services (1 replica) in production environments.</li> <li>If your workload does not tolerate interruptions well (e.g., caching proxies like Varnish and some databases).</li> </ul> <p>Explicitly targeting on-demand capacity type</p> <p>For most cases, you can safely skip this configuration and let the cluster provisioner determine the optimal capacity type. However, if your application falls into the categories mentioned earlier or there is a reasonable demand for on-demand instances, you can define this in your deployment using nodeSelectors.</p> Targeting on-demand capacity-type<pre><code>fullnameOverride: my-app\nreplicaCount: 2\nnodeSelector:\nkarpenter.sh/capacity-type: on-demand\n</code></pre> <p>This ensures that the workload always runs on on-demand instances.</p>"},{"location":"start/best-practices.html#pod-disruption-budgets","title":"Pod Disruption budgets","text":"<p>Pods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system software error. We call these unavoidable cases involuntary disruptions to an application. Examples are:</p> <ul> <li>a hardware failure of the physical machine backing the node</li> <li>cluster administrator deletes VM (instance) by mistake</li> <li>cloud provider or hypervisor failure makes VM disappear</li> <li>a kernel panic</li> <li>the node disappears from the cluster due to cluster network partition</li> <li>eviction of a Pod due to the node being out-of-resources.</li> </ul> <p>Except for the out-of-resources condition, all these conditions should be familiar to most users; they are not specific to Kubernetes.</p> <p>We call other cases voluntary disruptions. These include both actions initiated by the application owner and those initiated by a Cluster Administrator. Typical application owner actions include:</p> <ul> <li>deleting the deployment or other controller that manages the pod</li> <li>updating a deployment's Pod template causing a restart</li> <li>directly deleting a Pod (e.g. by accident)</li> </ul> <p>Cluster administrator actions include:</p> <ul> <li>Draining a node for repair or upgrade.</li> <li>Draining a node from a cluster to scale the cluster up or down</li> <li>Removing a Pod from a node to permit something else to fit on that node.</li> </ul> <p>These actions might be taken directly by the cluster administrator, or by automation run by the cluster administrator.</p> <p>Kubernetes offers features to help you run highly available applications even when you introduce frequent voluntary disruptions.</p> <p>As an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions. For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum. A web front end might want to ensure that the number of replicas serving load never falls below a certain percentage of the total.</p> <p>Kubedeploy offers an option to define PodDisruptionBudget by configuring <code>podDisruptionBudget</code> values.</p> <p>Enabling podDisruptionBudget</p> podDisruptionBudget example<pre><code>fullnameOverride: my-app\nreplicaCount: 3\npodDisruptionBudget:\nenabled: true\nminAvailable: 2\n</code></pre> <p>In the example configuration above, we ensure that our Pod has 3 replicas at normal runtime. Configured PodDisruptionBudget requires that at least 2 replicas are always available during any voluntary disruptions.</p> <p>To learn more about Pod disruptions and PodDisruptionBudgets, please refer to the official documentation.</p>"},{"location":"start/best-practices.html#resource-requirements","title":"Resource requirements","text":"<p>Tip</p> <p>A key to effective deployment and resource management is setting appropriate resource requests and limits on the containers running within a pod.</p> <p>By default no resource requests and limits are imposed on Pods and containers unless otherwise specified by namespace LimitRange policy. As a result, any Pod is allowed to consume as many resources as it requires or as are available on the node. Additionally, the scheduler has no insight into how many resources a pod needs and assumes that all Pods can fit on a single node unless other pod placement rules are configured.</p> <p>This creates the possibility of resource starvation on the node and some involuntary Pod disruptions. To avoid such situations it is recommended to define at least resource requests for Pods.</p> <p>Resource requests vs limits</p> <p>Resource requests reserve the specified resources for the execution of a Pod. Once reserved, the Kubernetes scheduler considers these resources unusable for scheduling new workloads on that node.</p> <p>Resource limits establish upper bounds on the resources a Pod can actually use. If limits are unspecified, a Pod can consume all available (unreserved) resources on the node where it's scheduled.</p> <p>Example</p> Defining resource requirements<pre><code>resource:\nrequests:\ncpu: 1\nmemory: 512Mi\nlimits:\ncpu: 2\nmemory: 1024Mi\n</code></pre> <p>In this example, the container reserves 1 CPU core and 512MB of memory for its execution. This reservation is guaranteed at all times. During scheduling, this Pod will not be placed on nodes with fewer resources than requested.</p> <p>Simultaneously, the container can burst up to 2 CPU cores (after which it will be throttled) and utilize up to 1024MB of memory. If the memory limit is reached and the container requests more memory from the system, the out-of-memory killer (OOM) will terminate it.</p> <p>In a scenario where a container with the aforementioned resource requests and limits is placed on a node with only enough capacity for its requests but not its limits, CPU usage will be throttled. Additionally, in cases of memory pressure, the pod will be evicted from the node in order to find a more suitable node with more available resources.</p> <p>Tip</p> <p>There's no one-size-fits-all rule for defining resource requests and limits. It largely involves a balancing act among actual requirements, workload availability, and platform cost optimization. As a general guideline, start with smaller requests and larger limits. Then, observe the workload patterns in your monitoring dashboards and make adjustments to the limits.</p> <p>Defining excessively large resource requests will result in a larger cluster and subsequently higher operating costs.</p> <p>Kubedeploy offers the ability to define resource requirements for each of its container components (main container, additionalContainers, and initContainers).</p> <p>Example</p> Defining resource requests<pre><code>fullnameOverride: my-app\nresources:\nrequests:\ncpu: 0.1\nmemory: 128Mi\nlimits:\ncpu: 1\nmemory: 512Mi\n</code></pre> <p>If you don't know your application requiremetns, this example provides a solid starting point for defining resource requirements for the main container. After some time, review the resource dashboards in your monitoring tool for this workload and fine-tune the requests and limits accordingly.</p> <p>For further insights into resource management for pods and containers, please refer to the official documentation page.</p>"},{"location":"start/best-practices.html#autoscaling","title":"Autoscaling","text":"<p>Tip</p> <p>Autoscaling can aid in maintaining low resource and platform cost utilization while retaining the capability to automatically enhance application throughput based on various scaling triggers.</p> <p>Autoscaling features within Kubedeploy enable the utilization of either Kubernets' built in Horizontal Pod Autoscaler or Keda.</p> <p>Configuration options are available under the <code>autoscaling</code> and <code>keda</code> values.</p> <p>Note</p> <p>You can only use one of the options, either HPA or Keda. HPA serves as a simpler autoscaling solution, which we will cover on this page. On the other hand, Keda offers a more intricate, fine-grained scaling solution; however, its configuration is beyond the scope of this document.</p> <p>Warning</p> <p>For HPA to function correctly, pods must define at least resource requests, as they are used in calculating utilization based on the CPU and memory utilization of the pods. While HPA can be deployed for deployments without defined resource requests, scaling will remain disabled until resource requests are defined.</p> <p>Define autoscaling policy</p> Simple autoscaling<pre><code>fullnameOverride: my-app\nresources:\nrequests:\ncpu: 0.5\nmemory: 128Mi\nautoscaling:\nenabled: true\nminReplicas: 2\nmaxReplicas: 10\ntargetCPUUtilizationPercentage: 80\n</code></pre> <p>In the above example we are defining deployment with resource requirements of 1/2 of CPU core.</p> <p>HPA is then configured to watch the CPU utilization of this workload. If it hits 80% of requested CPU resources it will increase the number of replicas by one until average deployment CPU utilization either drops below <code>80%</code> or <code>maxReplicas</code> is reached.</p> <p>HPA will attempt to downscale underutilized deployments every 5 minutes if the average CPU utilization is below 80%.</p> <p>Imporant</p> <p>It's important to note that the utilization percentage is always calculated from resource requests, not limits. If you have higher limits, you can, for example, define a percentage of 200%, which would allow pods to burst beyond their resource requests before triggering scaling.</p>"},{"location":"start/best-practices.html#pod-security-context","title":"Pod security context","text":"<p>The securityContext field allows you to set various security-related options for the containers within a pod. Some of the options that can be set include:</p> <ul> <li><code>runAsUser</code>: Sets the user ID (UID) that the container should run as. This can help to prevent privilege escalation attacks.</li> <li><code>runAsGroup</code>: Sets the group ID (GID) that the container should run as.</li> <li><code>capabilities</code>: Specifies the Linux capabilities that the container should have.</li> <li><code>privileged</code>: Indicates whether the container should be run in privileged mode or not.</li> <li><code>seLinuxOptions</code>: Specifies the SELinux context that the container should run with.</li> <li><code>fsGroup</code>: Sets the GID that the container\u2019s filesystem should be owned by.</li> </ul> <p>To gain a deeper understanding of how the securityContext options behave in different situations, refer to the official Kubernetes documentation. It outlines the intended behavior of each option, taking into consideration various factors such as the underlying operating system and container runtime being used, as well as the configuration of the Kubernetes cluster itself.</p> <p>By studying the documentation, you can gain insights into how these options can provide additional security controls for containers running within a Kubernetes pod. However, misconfiguring these options can lead to unintended consequences, such as preventing a container from running correctly or opening up security vulnerabilities, so read carefully and follow best practices when setting up these security controls.</p> <p>Kubedeploy offers configuration value <code>podSecurityContext</code> that can be used define desired security context applied to all containers in a Pod or <code>securityContext</code> which will be applied only to main container.</p> <p>Example</p> Pod security context<pre><code>fullnameOverride: my-app\npodSecurityContext:\nrunAsUser: 1000\nrunAsGroup: 3000\nfsGroup: 2000\n</code></pre> <p>In the configuration above, the runAsUser field specifies that for any Containers in the Pod, all processes run with user ID 1000.</p> <p>The runAsGroup field specifies the primary group ID of 3000 for all processes within any containers of the Pod. If this field is omitted, the primary group ID of the containers will be root(0).</p> <p>Any files created will also be owned by user 1000 and group 3000 when runAsGroup is specified. Since fsGroup field is specified, all processes of the container are also part of the supplementary group ID 2000. The owner for volume /data/demo and any files created in that volume will be Group ID 2000.</p>"},{"location":"start/best-practices.html#healthchecks-and-pod-lifecycle","title":"Healthchecks and Pod lifecycle","text":"<p>Defining proper health check probes can enhance the robustness of application deployments, mitigating potential downtimes and errors during rolling updates. Combined with Pod lifecycle hooks and minReadySeconds, we can fortify our application, minimizing runtime errors during evictions.</p>"},{"location":"start/best-practices.html#healthcheck-probes","title":"Healthcheck Probes","text":"<p>Many applications, when running for extended periods, eventually encounter broken states from which they cannot recover except through a restart. To address such situations, Kubernetes offers liveness probes for detection and remediation.</p> <p>In some cases, dealing with legacy applications demands an additional startup time during their initial initialization. In such scenarios, configuring liveness probe parameters can be challenging without compromising the swift response to application deadlocks that necessitated such a probe. The solution is to establish a startup probe using the same command, HTTP, or TCP check, with a failureThreshold * periodSeconds duration covering the worst-case startup time.</p> <p>Furthermore, applications might occasionally be unable to serve traffic temporarily. This could occur during startup when loading substantial data or configuration files, or when dependent on external services post-startup. In such instances, you don't want to terminate the application, but you also don't want to direct requests to it. Kubernetes provides readiness probes to detect and mitigate these scenarios. A pod with containers indicating they are not ready won't receive traffic through Kubernetes Services.</p> <p>Kubedeploy offers the possibility to define custom health check probes via <code>healthcheck</code> configuration value, both for the main container and  additional containers.</p> <p>Example</p> Main container healthcheck<pre><code>fullnameOverride: my-app\nports:\n- name: liveness-port\ncontainerPort: 8080\nprotocol: TCP\nhealthcheck:\nenabled: true\nprobes:\nlivenessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 5\nperiodSeconds: 10\nreadinessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 1\nperiodSeconds: 10\nstartupProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 30\nperiodSeconds: 10\n</code></pre> <p>In the example above we have exposed main containers port <code>8080</code> with name <code>liveness-port</code>. We then configured three healthcheck probes. The liveness probe generates HTTP request on liveness-port at /healthz URI every 10 seconds. If there are five consecutive request errors, the container will be restarted. The liveness probe will become effective onlyly after startupProbe returns a healthy state.</p> <p>Since the application might take some time to start up, we create startupProbe with same URI endpoint checks. However, we tolerate 30 failures before the container is restarted.</p> <p>Additionally, we've established a readinessProbe to remove the pod from load balancing if even one failure is detected. This helps avoid failed HTTP requests before the application restarts after five failures.</p> <p>Note</p> <p>Healthcheck probes can also be configured as commands that can be executed within container.</p> <p>To learn more about Kubernetes health check probes please see the official documentation.</p>"},{"location":"start/best-practices.html#minreadyseconds","title":"minReadySeconds","text":"<p>When combined with defined <code>readinessProbe</code>, minReadySeconds is used to signal the scheduler when the old replica versions can be removed during rolling version upgrades.</p> <p>Defining minReadySeconds with Kubedeploy</p> minReadySeconds<pre><code>fullnameOverride: my-app\nports:\n- name: liveness-port\ncontainerPort: 8080\nprotocol: TCP\nhealthcheck:\nenabled: true\nprobes:\nlivenessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 5\nperiodSeconds: 10\nreadinessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 1\nperiodSeconds: 10\nminReadySeconds: 120\n</code></pre> <p>In the example above, the container is deemed ready and begins receiving requests through the service object as soon as the readinessProbe reports a healthy status. Additionally, we've defined minReadySeconds to increase the time before the container is considered available.</p> <p>How Does This Impact Rolling Updates?</p> <ul> <li>When a rolling version update occurs, the Kubernetes scheduler respects any pod disruption budgets and rolling update strategies.</li> <li>By default, a new replicaSet is created with the new version. A new pod is then generated by the new replicaSet.</li> <li>Once the new pod is ready as indicated by the <code>readinessProbe</code>, it is included in service load balancing and starts receiving new traffic.</li> <li>The <code>minReadySeconds</code> timer waits for an additional <code>120 seconds</code> to ascertain whether the pod might crash with the influx of new traffic.</li> <li>This timer resets upon any crash. After the <code>minReadySeconds</code> duration passes, the old replicaSet's pod is removed.</li> </ul> <p>This process repeats until all replicas are updated to the new version. By employing <code>minReadySeconds</code>, applications become more resilient and less error-prone during rolling updates.</p>"},{"location":"start/best-practices.html#lifecycle-hooks","title":"Lifecycle Hooks","text":"<p>Kubernetes supports the postStart and preStop events. Kubernetes sends the postStart event immediately after a Container is started, and it sends the preStop event immediately before the Container is terminated. A Container may specify one handler per event.</p> <p>Defining custom lifecycle hook with Kubedeploy</p> minReadySeconds<pre><code>fullnameOverride: my-app\nimage:\nterminationGracePeriodSeconds: 120\nlifecycle:\npreStop:\nexec:\ncommand: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"]\nhealthcheck:\nenabled: true\nprobes:\nreadinessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 1\nperiodSeconds: 10\n</code></pre> <p>In this example, we've registered a <code>preStop</code> hook triggered just before the container terminates. Kubernetes awaits the completion of this command or until the <code>terminationGracePeriod</code> is reached before terminating the container.</p> <p>The command in the example sends a graceful shutdown signal to nginx, allowing it to stop accepting new requests while completing ongoing ones.</p> <p>Consequently, the readiness probe will fail, and the pod will no longer be actively load balanced by the service object.</p> <p>Any long-running requests will be finalized, and the command will exit, enabling a clean and graceful pod termination without generating failed HTTP requests.</p> <p>To learn more about lifecycle hooks please see the official documentation.</p>"},{"location":"start/changelog.html","title":"CHANGELOG","text":""},{"location":"start/changelog.html#122","title":"1.2.2","text":"<p>(2024-01-06)</p>"},{"location":"start/changelog.html#bug-fixes","title":"Bug Fixes","text":"<ul> <li>extraVolumeMounts:  add missing features (999c3e69)</li> <li>test:  extraVolumeMounts tests were not included (3c4dece9)</li> </ul>"},{"location":"start/changelog.html#features","title":"Features","text":"<ul> <li>extraVolumeMounts:  add chartName parameter (694fac66)</li> </ul>"},{"location":"start/changelog.html#docs","title":"Docs","text":"<ul> <li>extraVolumeMounts:  extend documentation on subpath and items (20b02842)</li> </ul>"},{"location":"start/changelog.html#121","title":"1.2.1","text":"<p>(2023-12-17)</p>"},{"location":"start/changelog.html#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>podExtraLabels:  apply extra labels only to pod templates (6f71f4b7)</li> </ul>"},{"location":"start/changelog.html#120","title":"1.2.0","text":"<p>(2023-12-02)</p>"},{"location":"start/changelog.html#docs_1","title":"Docs","text":"<ul> <li>extraingress:  add extraingress docs (a92fb68b)</li> </ul>"},{"location":"start/changelog.html#features_1","title":"Features","text":"<ul> <li>autoscaling:<ul> <li>add configurable behavior (c8afefb7)</li> <li>enable autoscaling for Statefulset (e780f5d7)</li> </ul> </li> <li>command,args:  use helper functions (b9855991)</li> <li>extraIngress:  add support for extraIngress objects (518e7f30)</li> <li>jobspec:  add ttlSecondsAfterFinished default/configurable value (6fffc81d)</li> <li>keda:  add scaling support for StatefulSets (9ad4f00e)</li> <li>service:  add automatic headless service (c6b12bdd)</li> </ul>"},{"location":"start/changelog.html#111","title":"1.1.1","text":"<p>(2023-10-22)</p>"},{"location":"start/changelog.html#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>healthcheck:  allow optional healthcheck probes (c51060eb)</li> </ul>"},{"location":"start/changelog.html#docs_2","title":"Docs","text":"<ul> <li>minor documentation updates (e732452b)</li> </ul>"},{"location":"start/changelog.html#110","title":"1.1.0","text":"<p>(2023-09-16)</p> <p>Public chart documentation is now available, featuring Quickstart guide, full documentation per each configuration value,  as well as deployment examples.</p>"},{"location":"start/changelog.html#new-features","title":"New features","text":"<ul> <li>added: extraObjects</li> <li>added: envFrom</li> <li>added: headless service support</li> <li>added: configMapsHash</li> <li>added: ingress - svcPort for targeting backends</li> <li>added: ingress - per path overrides for svcPort and pathType</li> <li>added: ingress - withSSL support (automatic tls spec configuration)</li> <li>added: podExtraLabels</li> <li>added: extraVolumeMounts</li> <li>added: extraSecrets</li> <li>added: unit tests for each top-level values.yaml configuration key</li> </ul>"},{"location":"start/changelog.html#changes","title":"Changes","text":"<ul> <li>change: split deployment tests</li> <li>fixed: empty lines in volumeMounts</li> <li>change: ingress - make path in host optional</li> <li>change: statefulsets template will use kubedeploy.kubeVersion function when checking for Kubernetes version</li> <li>change: ingress - host in hosts list is now required.</li> <li>change: ingress - removed default dummy domain from ingress hosts</li> <li>change: ingress - ingress.tls is now empty and optional in favor of withSSL. If defined by user it will override withSSL configuration.</li> <li>change: ingress - template will use kubedeploy.kubeVersion function when checking for Kubernetes version</li> <li>change: service - default ports are now empty and optional. Template will try to autodetect container ports if service.ports are unspecified</li> </ul>"},{"location":"start/changelog.html#fixes","title":"Fixes","text":"<ul> <li>bugfix: use standardized label selectors for chart managed NetworkPolicy objects</li> <li>bugfix: set default value (healthcheck.enabled: false) for additoinalContainers healthcheck if unspecified</li> <li>bugfix: autoscaling HPA object spec bug</li> <li>bugfix: ingress - will now respect path under hosts list (previusly always defaulted to /)</li> <li>bugfix: affinity will now disable automatic podAntiAffinity rules reducing possibility of rule collisions.</li> </ul>"},{"location":"start/changelog.html#100","title":"1.0.0","text":""},{"location":"start/changelog.html#breaking-changes","title":"Breaking changes","text":"<ul> <li>Persistent volumes are now available only with statefulsets. Previous version of chart allowed for persistent volumes with deploymentMode: Deployment when replicaCount was less than 2. From version 1.0.0 persistent volumes will be supported only for statefulsets.</li> <li>configMaps in version 0.8.0 where not generating unique names across releases. Starting from version 1.0.0 defined configmap names will have their final name prefixed using fullname helper function.</li> </ul>"},{"location":"start/changelog.html#new-features_1","title":"New Features","text":"<ul> <li>added support for defining NetworkPolicy objects (https://kubernetes.io/docs/concepts/services-networking/network-policies/) by using .Values.networkPolicy. If networkpolicies are enabled, chart will automatically add ingress rules for monitoring and ingress</li> <li>it is now possible to define multiple containers in pods by using .Values.additionalContainers</li> <li>extended support for defining fine grained resource options for each container in .Values.additionalContainers and .Values.initContainers</li> <li>added support for mounting configmaps as volumes in pods by using .Values.configMaps[].mount: True and defining .Values.configMaps[].mountPath</li> <li>added deploymentMode of type Cronjob. See .Values.cronjobspec for more details</li> <li>added support for defining minReadySeconds for Deployments and Statefulsets</li> <li>added support for defining topologySpreadConstraints</li> <li>added .Values.podAntiAffinity and .Values.podAntiAffinityTopologyKey for easier definition of pod antiaffinity rules.</li> <li>added support for defining main container lifecycle hooks in .Values.image.lifecycle</li> <li>other code improvements and bug fixes</li> </ul>"},{"location":"start/changelog.html#090","title":"0.9.0","text":"<ul> <li>added KEDA 2.x support</li> </ul>"},{"location":"start/changelog.html#082","title":"0.8.2","text":"<ul> <li>add option to disable automatic healthcheck probes</li> </ul>"},{"location":"start/changelog.html#081","title":"0.8.1","text":"<p>Breaking change - Use fullname helper for defining app.kubernetes.io/name. This fixes scenario where app.kubernetes.io/name would always be kubedeploy if nameOverride is not set. In this fix, fullNameOverride will be used first, then nameOverride if present, if none are present, use release name + chart name as app name label - Use image.tag value if available for app.kubernetes.io/version</p>"},{"location":"start/changelog.html#080","title":"0.8.0","text":"<ul> <li>added support for defining ConfigMap objects from values</li> </ul>"},{"location":"start/changelog.html#071","title":"0.7.1","text":"<ul> <li>fixed bug with monitoring label matchers</li> <li>fixed bug with default scrapeTimeout value</li> </ul>"},{"location":"start/changelog.html#070","title":"0.7.0","text":"<ul> <li>added support for enabling monitoring via prometheus operator.</li> <li>cleaned up helmchart documentation for easier readability</li> </ul>"},{"location":"start/changelog.html#060","title":"0.6.0","text":"<ul> <li>added support for overriding healthcheck probes</li> </ul>"},{"location":"start/changelog.html#051","title":"0.5.1","text":"<ul> <li>added support for multiple init containers in Job, Deployment and Statefulset deployment mode</li> </ul>"},{"location":"start/changelog.html#050","title":"0.5.0","text":"<ul> <li>added support for init container in Job, Deployment and Statefulset deployment mode</li> </ul>"},{"location":"start/changelog.html#044","title":"0.4.4","text":"<ul> <li>don't set port 80 on containers by default</li> <li>deploy liveness and readiness probes only when http port name is defined</li> </ul>"},{"location":"start/changelog.html#043","title":"0.4.3","text":"<ul> <li>fix ingress backend port targeting</li> </ul>"},{"location":"start/changelog.html#042","title":"0.4.2","text":"<ul> <li>fixed wrongly nested command option in deployment and statefulset</li> </ul>"},{"location":"start/changelog.html#041","title":"0.4.1","text":"<ul> <li>added support for image commands</li> </ul>"},{"location":"start/changelog.html#040","title":"0.4.0","text":"<ul> <li>added support for job workloads</li> <li>added support for statefulset workloads</li> <li>minor bug fixes</li> </ul>"},{"location":"start/changelog.html#032","title":"0.3.2","text":"<ul> <li>fixed container name to full name instead of chart name</li> </ul>"},{"location":"start/changelog.html#031","title":"0.3.1","text":"<ul> <li>added public docs and chart home url</li> </ul>"},{"location":"start/changelog.html#030","title":"0.3.0","text":"<ul> <li>added support for defining container and service ports</li> <li>added podDisruptionBudget support</li> <li>added container env varialbes support</li> <li>updated chart metadata</li> </ul>"},{"location":"start/changelog.html#021","title":"0.2.1","text":"<p>Initial release</p>"},{"location":"start/quickstart.html","title":"Quickstart","text":"<p>To deploy with Kubdeploy you will require Helm, and Kubedeploy's Helm repo.</p> <pre><code>helm repo add sysbee https://charts.sysbee.io/stable/sysbee\nhelm repo update\n</code></pre> <p>This section covers how to deploy and modify the deployment of our containerized application by modifying default Kubedeploy values.</p> <p>Values overrides</p> <p>Charts values can be customized either by using <code>--set</code> on the command line or by using a custom <code>values.yaml</code> file and passing it to <code>helm install</code> command. It is recommended to use a <code>values.yaml</code> file as some overrides might be to complex for <code>--set</code> on command line</p>"},{"location":"start/quickstart.html#deploying-simple-application","title":"Deploying simple application","text":"<p>We can easily deploy any containerized application by specifying a custom repository as a configurable value.</p> Values fileCLI values.yaml<pre><code>image:\nrepository: nginx\n</code></pre> Deploy command<pre><code>helm install nginx sysbee/kubedeploy -f values.yaml\n</code></pre> Deploy command<pre><code>helm install nginx sysbee/kubdeploy --set image.repository=nginx\n</code></pre>"},{"location":"start/quickstart.html#defining-image-version","title":"Defining image version","text":"<p>Default application version</p> <p>If unspecified, Kubedeploy will use <code>latest</code> as image tag</p> <p>If we don't want to run our deployment from the latest tag, we can easily specify desired app version as well:</p> Values fileCLI values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\n</code></pre> Deploy command<pre><code>helm install nginx sysbee/kubedeploy -f values.yaml\n</code></pre> Deploy command<pre><code>helm upgrade --install nginx sysbee/kubedeploy \\\n--set image.repository=nginx \\\n--set image.tag=1.25.2\n</code></pre>"},{"location":"start/quickstart.html#changing-deployment-modes","title":"Changing deployment modes","text":"<p>DeploymentModes</p> <p>By default, Kubedeploy will create Kubernetes Deployment for your container image.</p> <p>We can also deploy our image as StatefulSets:</p> Values fileCLI values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Statefulset\n</code></pre> Deploy command<pre><code>helm install nginx sysbee/kubedeploy -f values.yaml\n</code></pre> Deploy command<pre><code>helm upgrade --install nginx sysbee/kubedeploy \\\n--set image.repository=nginx \\\n--set image.tag=1.25.2 \\\n--set deploymentMode=Statefulset\n</code></pre>"},{"location":"start/quickstart.html#persistence","title":"Persistence","text":"<p>Persistence support</p> <p>Changing <code>deploymentMode</code> to Statefulset will not enable persistence by default</p> <p>We can define persistent storage and its size by adjusting <code>persistency</code> values:</p> Values fileCLI values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Statefulset\npersistency:\nenabled: true\ncapacity:\nstorage: 10Gi\n</code></pre> Deploy command<pre><code>helm install nginx sysbee/kubedeploy -f values.yaml\n</code></pre> Deploy command<pre><code>helm upgrade --install nginx sysbee/kubedeploy \\\n--set image.repository=nginx \\\n--set image.tag=1.25.2 \\\n--set deploymentMode=Statefulset \\\n--set persistency.enabled=true --set persistency.capacity.storage=10Gi\n</code></pre> Can I use persistence with Deployments? <p>Kubedeploy supports persistency only for StetefulSets. However, you will learn in advanced examples how to enable persistent volumes even for Deployments.</p>"},{"location":"start/quickstart.html#exposing-the-application","title":"Exposing the application","text":"<p>Up until now, we have added Nginx as an application in our Kubernetes cluster. It's time to expose it on public domain:</p> Values fileCLI values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Statefulset\npersistency:\nenabled: true\ncapacity:\nstorage: 10Gi\ningress:\nenabled: true\nhosts:\n- mydomain.com\nports:\n- containerPort: 80 # (1)\n</code></pre> <ol> <li>Opens port <code>80</code> for container. Ingress will automatically use first container port for routing traffic.</li> </ol> Deploy command<pre><code>helm install nginx sysbee/kubedeploy -f values.yaml\n</code></pre> Deploy command<pre><code>helm upgrade --install nginx sysbee/kubedeploy \\\n--set image.repository=nginx \\\n--set image.tag=1.25.2 \\\n--set deploymentMode=Statefulset \\\n--set persistency.enabled=true --set persistency.capacity.storage=10Gi \\\n--set ingress.enabled=true --set ingress.hosts[0].host=mydomain.com \\\n--set ports[0].containerPort=80 #(1)\n</code></pre> <ol> <li>Opens port <code>80</code> for container. Ingress will automatically use first container port for routing traffic.</li> </ol>"},{"location":"start/quickstart.html#whats-next","title":"What's next?","text":"<p>Check out Best practices and Examples by Values sections for more currated examples on customizable values in Kubedeploy chart.</p> <p>Look into Reference section for Kubedeploy Changelog or Values for more information.</p>"},{"location":"start/values/additionalcontainers.html","title":"additionalContainers","text":"<p>Feature state:  1.0.0</p> <p>A Pod can have multiple containers running apps within it.</p> <p>Pods are designed to support multiple cooperating processes (as containers) that form a cohesive unit of service. The containers in a Pod are automatically co-located and co-scheduled on the same physical or virtual machine in the cluster. The containers can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated.</p> <p><code>additionalContainers</code> value in Kubedeploy allow for defining one or more additionalContainers that will run alongside main application container.</p> <p>Note</p> <p>By default, Kubedeploy does not specify any <code>additionalContainers</code></p> <p>Supported <code>additionalContainers</code> values in Kubedeploy (click on the plus sign for more info):</p> <pre><code>additionalContainers:\nenabled: false # (1)\npullPolicy: IfNotPresent # (2)\nresources: {} # (3)\ncontainers: # (4)\n- name: \"busybox-init\" # (5)\nrepository: busybox # (6)\ntag: \"latest\" #(7)\ncommand: [\"sh\", \"-c\", \"exit 0\"] # (8)\nargs: [] # (9)\nports: [] # (10)\nhealthcheck:\nenabled: false # (11)\nprobes:\nlivenessProbe: {}\nreadinessProbe: {}\nstartupProbe: {}\nresources: {} # (12)\n</code></pre> <ol> <li>Define if initContainers are enabled.</li> <li>initContainers image pull policy</li> <li> <p>(optional) Define initContainers global resource requests and limits. Will be applied to all initContainers if more specific (per container) resource requests and limits are not defined.</p> <p>Example</p> <pre><code>initContainers:\nresources:\nrequests:\ncpu: 0.1\nmemory: 128Mi\nlimits:\ncpu: 0.2\nmemory: 256Mi\n</code></pre> </li> <li> <p>(list) Sequential list of initContainers</p> </li> <li>(required) Define initContainer name</li> <li>(required) Define initContainer repository</li> <li>(optional) Define initContainer image tag, defaults to <code>latest</code> if unspecified</li> <li>(optional) Define custom command for initContainer</li> <li>(optional) Define custom arguments for initContainer</li> <li> <p>(optional) Define additionalContainer exposed ports (see: containerPort)</p> <p>Example</p> <pre><code>additionalContainers:\nenabled: true\ncontainers:\n- name: busybox\nrepository: busybox\nports:\n- containerPort: 9080\nname: extra\nprotocol: TCP\n</code></pre> </li> <li> <p>(optional) Enable custom healthcheck probes for additionalContainer</p> </li> <li>(optional) Define custom resources for this specific additionalContainer</li> </ol> <p>Additional containers can be used to augment application with additional functionality, for example defining extra container metric exporter, or using reverse authenticaton proxy.</p> <p>Shared values/volumes</p> <p><code>additionalContainers</code> will automatically inherit all the environment values set in <code>env</code>, <code>envFrom</code> and volume mounts defined in <code>configMaps</code>, <code>extraSecrets</code> and <code>extraVoumeMounts</code> just as the main container would.</p> <p>Define additionalContainers that exports metrics</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nports:\n- containerPort: 80\nname: http\nprotocol: TCP\nadditionalContainers:\nenabled: true\nresources:\nrequests:\ncpu: 0.1\nmemory: 128Mi\nlimits:\ncpu: 0.2\nmemory: 256Mi\ncontainers:\n- name: metrics-exporter\nrepository: nginx/nginx-prometheus-exporter\ntag: latest\nargs:\n- -nginx.scrape-uri=http://localhost:80/stub_status\nports:\n- containerPort: 9113\nname: metrics\nprotocol: TCP\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>See also:</p> <ul> <li>additionalContainers</li> <li>env</li> <li>envFrom</li> <li>configMaps</li> <li>extraSecrets</li> <li>extraVolumeMouts</li> </ul>"},{"location":"start/values/affinity.html","title":"affinity","text":"<p><code>affinity</code> value enables defining of raw affinity rules.</p> <p>When defined, it disables/overrides automatic podAntiAffinity rules.</p> <p>Note</p> <p>By default <code>affinity</code> is undefined.</p> <p>Define custom affinity rules</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\naffinity:\npodAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app.kubernetes.io/instance\noperator: In\nvalues:\n- backend-app\ntopologyKey: topology.kubernetes.io/hostname\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example three replicas of application would require to be scheduled on Kubernetes nodes that also host the application with instance name backend-app.</p> <p>For more examples on custom affinity rules, please read the official documentation.</p> <p>See also:</p> <ul> <li>podAntiAffinity</li> <li>podAntiAffinityTopologyKey</li> <li>nodeSelector</li> <li>topologySpreadConstraints</li> <li>Best practices</li> </ul>"},{"location":"start/values/autoscaling.html","title":"autoscaling","text":"<p><code>autoscaling</code> value in Kubedeploy allows controlling the parameters for Horizontal Pod Autoscaler object.</p> <p>Autoscaling can aid in maintaining low resource and platform cost utilization while retaining the capability to automatically enhance application throughput based on various scaling triggers.</p> <p>Warning</p> <p>For HPA to function correctly, pods must define at least resource requests, as they are used in calculating utilization based on the CPU and memory utilization of the pods. While HPA can be deployed for deployments without defined resource requests, scaling will remain disabled until resource requests are defined.</p> <p>Available values for <code>autoscaling</code> in Kubedeploy:</p> <pre><code>autoscaling:\nenabled: false # (1)\nminReplicas: 1 # (2)\nmaxReplicas: 10 # (3)\ntargetCPUUtilizationPercentage: 80 # (4)\ntargetMemoryUtilizationPercentage: # (5)\nbehavior: {} # (6)\n</code></pre> <ol> <li>Enable autoscaling. Works only with Deployment and Statefulset deploymentMode</li> <li>Minimum number of Pod replicas</li> <li>Maximum number of Pod replicas</li> <li>(int) Scaling target CPU utilization as percentage of resources.requests.cpu</li> <li>(int) Scaling target memory utilization as percentage of resources.requests.mem</li> <li>(Feature state:  1.2.0) (map) Define custom scaling behavior</li> </ol> <p>Define simple autoscaling policy</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nresources:\nrequests:\ncpu: 0.5\nmemory: 128Mi\nautoscaling:\nenabled: true\nminReplicas: 2\nmaxReplicas: 10\ntargetCPUUtilizationPercentage: 80\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example we are defining deployment with resource requirements of 1/2 of CPU core.</p> <p>HPA is then configured to watch the CPU utilization of this workload. If it hits 80% of requested CPU resources it will increase the number of replicas by one until average deployment CPU utilization either drops below <code>80%</code> or <code>maxReplicas</code> is reached.</p> <p>HPA will attempt to downscale underutilized deployments every 5 minutes if the average CPU utilization is below 80%.</p> <p>Imporant</p> <p>It's important to note that the utilization percentage is always calculated from resource requests, not limits. If you have higher limits, you can, for example, define a percentage of 200%, which would allow pods to burst beyond their resource requests before triggering scaling.</p> <p>See also:</p> <ul> <li>resources</li> </ul>"},{"location":"start/values/configmaps.html","title":"configMaps","text":"<p>Feature state:  0.8.0</p> <p><code>configMaps</code> value in Kubedeploy allow for deploying multiple custom configMaps objects.</p> <p>It is also possible to automatically mount ConfigMaps in all containers of a Pod by definig custom mount parametars. (Feature state:  1.0.0)</p> <p>Note</p> <p>By default <code>configMaps</code> in Kubedeploy will not deploy any ConfigMap objects.</p> <p>However, when defining them under this value, chart will automatically prepend <code>kubedeploy.fullname</code> to any defined configmap object to prevent collisions with other releases.</p> <p>Define custom configMaps</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nconfigMaps:\n- name: configmap1\nmount: True # (optional) Should this configmap be mounted as volume in containers?\nmountPath: /data/confmap # (required if mount=True) Define mount path for this configmap\ndata:\nkubedeploy.txt: |+\nconfigmap values\n- name: configmap2\ndata:\nconfig: |+\nconfig2\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>As a result of the above example, Kubdeploy will create two extra ConfigMap objects named: <code>webapp-my-app-configmap1</code> and <code>webapp-my-app-configmap2</code>.</p> <p>First configmap will also be mounted inside all of the Pod's containers at <code>/data/configmap</code> path exposing <code>kubedeploy.txt</code> as file on <code>/data/configmap/kubedeploy.txt</code>.</p> <p>Tip</p> <p>Common usecase in the above scenario would be creating and automatically mounting any configuration files your application might need during its runtime.</p> <p>See also:</p> <ul> <li>configMapsHash</li> <li>extraSecretes</li> </ul>"},{"location":"start/values/configmapshash.html","title":"configMapsHash","text":"<p>Feature state:  1.1.0</p> <p><code>configMapsHash</code> value in Kubedeploy enables custom annotation on Deployment and Statefulsets. Hash is automatically calculated based on the contents of deployed ConfigMaps.</p> <p>Note</p> <p><code>configMapsHash</code> in Kubedeploy is disabled by default.</p> <p>enabling this feature will trigger Pod rolling restarts whenever content of configMaps data or name is changed. This can be useful if the application can't detect any mounted ConfigMap file content changes.</p> <p>Enable configMapsHash</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nconfigMaps:\n- name: configmap1\nmount: True # (optional) Should this configmap be mounted as volume in containers?\nmountPath: /data/confmap # (required if mount=True) Define mount path for this configmap\ndata:\nkubedeploy.txt: |+\nconfigmap values\n- name: configmap2\ndata:\nconfig: |+\nconfig2\nconfigMapsHash: true\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>See also:</p> <ul> <li>configMaps</li> <li>extraSecretes</li> </ul>"},{"location":"start/values/cronjobspec.html","title":"cronjobspec","text":"<p>Feature state:  1.0.0</p> <p><code>cronjobspec</code> value in Kubedeploy defines CronJob specific parameters.</p> <p>Note</p> <p><code>cronjobspec</code> will be only taken into account if the <code>deploymetMode</code> is set to <code>Cronjob</code>.</p> <p>Available values for <code>cronjobspec</code> in Kubedeploy:</p> <pre><code>cronjobspec:\nschedule: \"0 * * * *\" # (1)\nrestartPolicy: OnFailure # (2)\ncommand: [] # (3)\nargs: [] # (4)\nbackoffLimit: 3 # (5)\nstartingDeadlineSeconds: 180 # (6)\nsuccessfulJobsHistoryLimit: 3 # (7)\nfailedJobsHistoryLimit: 1 # (8)\nconcurrencyPolicy: \"\" # (9)\n</code></pre> <ol> <li>Define cronjob schedule, for details see reference</li> <li>Define restart policy for cronjob if deploymentMode=Cronjob.</li> <li>Define command for cronjob</li> <li>Define args for cronjob</li> <li>Define job backoff limit, see reference</li> <li>(optional) Define deadline for starting the job, see reference</li> <li>Define number of successful Job logs to keep</li> <li>Define number of failed Job logs to keep</li> <li>Define concurrency policy options: Allow (default), Forbid or Replace, see reference</li> </ol> <p>Deprecation Warning</p> <p>Starting from Kubedeploy version 1.2, you should begin using image.command and image.args instead of cronjobspec.command and cronjobspec.args. These values will remain available as failsafe options until Kubedeploy 2.0, at which point they will be removed.</p> <p>Define cronjob</p> values.yaml<pre><code>nameOverride: my-cronjob\ndeploymentMode: Cronjob\nimage:\nrepository: busybox\ntag: latest\ncronjobspec:\nschedule: \"*/10 * * * *\"\ncommand: [\"sh\", \"-c\", \"echo hello from cronjob\" ]\n</code></pre> Deploy command<pre><code>helm install hello sysbee/kubedeploy -f values.yaml\n</code></pre> <p>The above example will create a CronJob that will run every 10 minutes and echo a hello message.</p> <p>See also:</p> <ul> <li>deploymentMode</li> <li>jobspec</li> </ul>"},{"location":"start/values/deploymentmode.html","title":"deploymentMode","text":"<p>Kubedeploy by default deploys your application in Kubernetes cluster as Deployment.</p> <p>You can also specify different <code>deplyomentMode</code> per use-case:</p> <ul> <li><code>Deployment</code> - default if unspecified</li> <li><code>Statefulset</code> - deploy application as StatefulSets</li> <li><code>Job</code> - deploy application as Jobs</li> <li><code>Cronjob</code> - deploy application as CronJob</li> <li><code>None</code> - do not deploy application. In this mode Kubedeploy can be used to deploy, ConfigMaps, Secrets, etc.</li> </ul> <p>Application as StatefulSets</p> values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Statefulset\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Additional options</p> <p>Some deployment modes offer additional mode specific customizations or restrictions, see related Kubedeploy values for more info.</p> <p>See also:</p> <ul> <li>cronjobspec</li> <li>jobspec</li> <li>persistency</li> </ul>"},{"location":"start/values/env.html","title":"env","text":"<p><code>env</code> value in Kubedeploy can be used to define environment variables that will be configured on all containers in a Pod.</p> <p>User can define containers env variable by passing the supported config.</p> <p>Note</p> <p>By default containers <code>env</code> is undefined.</p> <p>Define simple env var for container</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nenv:\n- name: NGINX_PORT\nvalue: 8080\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>We can also use extended version to define environment values from ConfigMap or Secret objects.</p> <p>Define env var from other sources</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nenv:\n# fetch value from ConfigMap\n- name: NGINX_PORT\nvalueFrom:\nconfigMapKeyRef:\nname: configmap-name\nkey: nginx-port\n# fetch value from Secret\n- name: NGINX_HOSTNAME\nvalueFrom:\nsecretKeyRef:\nname: secret-name\nkey: nginx-hostname\noptional: true\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Defining multiple env vars from other sources</p> <p>While the above example might work for referening couple of secret keys for extracting values, the configuration in values.yaml can be simplified by using <code>envFrom</code></p> <p>See also:</p> <ul> <li>envFrom</li> <li>extraSecrets</li> <li>configMaps</li> </ul>"},{"location":"start/values/envfrom.html","title":"envFrom","text":"<p>Feature state:  1.1.0</p> <p><code>envFrom</code> value in Kubedeploy can be used to define environment variables that will be configured on all containers in a Pod.</p> <p>For reference see envFrom secret example or envFrom configmap example</p> <p>Note</p> <p>By default containers <code>envFrom</code> is undefined.</p> <p>Define simple envFrom for containers</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nenvFrom:\n#env from ConfigMap\n- configMapRef:\nname: special-config\n# env from Secret\n- secretRef:\nname: test-secret\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In above example, envFrom will automatically define environment variables for all containers in a Pod by extracting all the ConfigMap/Secret keys as env var names, and their values as env var values.</p> <p>See also:</p> <ul> <li>env</li> <li>extraSecrets</li> <li>configMaps</li> </ul>"},{"location":"start/values/extraingress.html","title":"extraIngress","text":"<p>Feature state:  1.2.0</p> <p>extraIngress allows defining a list of one or multiple Ingress API objects, when there is need for more than one Ingress object per deployment.</p> <p>Note</p> <p>If you require just one Ingress object please see ingress configuration option.</p> <p>extraIngress mostly follows the config options present in ingress config with exception of name/enabled parametar as showcased below.</p> <p><code>extraIngress</code> value in Kubedeploy allows you to control parameters for Ingress object that would be created by the chart.</p> <p>Note</p> <p>By default <code>extraIngress</code> is disabled.</p> <p>Available values for <code>extraIngress</code> in Kubedeploy:</p> <pre><code>extraIngress:\n- name: ingress2 #(1)\nclassName: \"haproxy\" # (2)\npathType: ImplementationSpecific # (3)\nwithSSL: true # (4)\nsvcPort: \"\" # (5)\nannotations: # (6)\ncert-manager.io/cluster-issuer: letsencrypt\nhosts: # (7)\n- host: \"\" # (8)\npaths: [] # (9)\ntls: [] # (10)\n</code></pre> <ol> <li>(required) Name of extraIngress object</li> <li>(optional) Ingress class name. In case you have multiple Ingress controllers you can specifically target desired one. Defaults to \"haproxy\"</li> <li>(optional) Default Ingress pathType</li> <li>(optional) Deploy Ingress object with SSL support. Automatically configures the Ingress <code>tls</code> spec.</li> <li>(optional) Define default Service port that will be targeted by Ingress. If left undefined Ingress will use first port from <code>service.ports</code> (or first port from <code>ports</code>) to route traffic to.</li> <li>(optional) Additional Ingress annotations, can be used to define custom Ingress controller configuration. By default, it sets the cert-manager annotation to use <code>letsencrypt</code> cluster-issuer for issuing SSL certificates for configured ingress domain.</li> <li>(required) (list) Ingress host list.</li> <li>(required) (string) Define Ingress hostname</li> <li> <p>(optional) (list) Ingress host paths.     example usage:</p> <p>Example</p> <pre><code>extraIngress:\n- name: ingress2\nhosts:\n- host: \"my-domain.com\"\npaths:\n- path: /api # (optional) defaults to /\nsvcPort: 8080 # (optional) defaults to `ingress.svcPort`\npathType: Prefix # (optional) defaults to `ingress.pathType`\n</code></pre> </li> <li> <p>(optional) (list) Ingress TLS list.     overrides any auto configured tls config created by withSSL.</p> <p>Allows for custom secretName and host list to be defined. In case when you have pre-configured SSL stored in Kubernetes Secret. If secret does not exist, new one will be created by cert-manager.</p> <p>example usage:</p> <p>Example</p> <pre><code>extraIngress:\n- name: ingress2\nhosts:\n- host: \"my-domain.com\"\ntls:\n- hosts: # (required) list of hosts covered by ssl in secret\n- my-domain.com\nsecretName: my-domain-ssl # name of secret where SSL is configured\n</code></pre> </li> </ol> <p>See also:</p> <ul> <li>ingress</li> <li>ports</li> <li>service</li> <li>additionalContainers</li> <li>extraSecrets</li> </ul>"},{"location":"start/values/extraobjects.html","title":"extraObjects","text":"<p>Feature state:  1.1.0</p> <p><code>extraObjects</code> value in Kubedeploy accepts a list of raw Kubernetes manifests to be deployed.</p> <p>Note</p> <p><code>extraObjects</code> can be used to define any other Kubernetes object type or configuration that is not directly supported by Kubedeploy.</p> <p>Warning</p> <p>Object names deployed with <code>extraObjects</code> will not be prefixed with <code>kubedeploy.fullname</code> template. Extra care should be taken when deploying extraObjects with multiple releases to avoid Object name collisions.</p> <p>Define csi SecretProvider with extraObjects</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: 1.25.2\nextraObjects:\n- apiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\nname: aws-secrets-manager-secrets\nspec:\nprovider: aws\nparameters:\nobjects: |\n- objectName: \"name-of-aws-secret\"\nobjectType: \"secretsmanager\"\njmesPath:\n- path: db_username\nobjectAlias: db_username\n- path: db_password\nobjectAlias: db_password\n- path: db_hostname\nobjectAlias: db_hostname\n- path: database\nobjectAlias: database\nextraVolumeMounts:\n- name: external-secrets\nreadOnly: true\nmountPath: /etc/aws-secrets\ncsi:\ndriver: secrets-store.csi.k8s.io\nreadOnly: true\nvolumeAtributes:\nsecretProviderClass: \"aws-secretes-manager-secrets\"\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>The above example will create a SecretProviderClass object which can be referenced later in <code>extraVolumeMounts</code>.</p> <p>See also:</p> <ul> <li>extraVolumeMounts</li> </ul>"},{"location":"start/values/extrasecrets.html","title":"extraSecrets","text":"<p>Feature state:  1.1.0</p> <p><code>extraSecrets</code> value in Kubedeploy allow for deploying multiple custom Secrets objects.</p> <p>It also allows automatic mounting of Secretes in all containers of a Pod by definig cusom mount parametars.</p> <p>Note</p> <p>By default <code>extraSecrets</code> in Kubedeploy will not deploy any Secrets objects.</p> <p>However, when defining them under this value, chart will automatically prepend <code>kubedeploy.fullname</code> to any defined Secret object to prevent collisions with other releases.</p> <p>Define custom extraSecrets</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nextraSecrets:\n- name: opaque-secret\ntype: Opaque # (optional) - Default: Opaque if unspecified\nmount: True # (optional) - should this sercret be mounted as volume in containers\nmountPath: /mnt/secret-vol0 # (required if mount=True) - define mount path for this secret\ndata:\nkey: value # value will be automatically base64 encoded by chart template\n- name: tls-secret\ntype: kubernetes.io/tls\ndata:\ntls.crt: |\n--------BEGIN CERTIFICATE-----\nMIIC2DCCAcCgAwIBAgIBATANBgkqh ...\ntls.key: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...\ntls.ca: |\n--------BEGIN CERTIFICATE-----\nMIIC2DCCAcCgAwIBAgIBATANBgkqh ...\n- name: ssh-key-secret\ntype: kubernetes.io/ssh-auth\ndata:\nssh-privatekey: |\nMIIEpQIBAAKCAQEAulqb/Y ...\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>As a result of the above example, Kubdeploy will create three extra Secrets objects named: <code>webapp-my-app-opaque-secret</code>,  <code>webapp-my-app-tls-secret</code> and <code>webapp-my-app-ssh-key-secret</code>.</p> <p>First Secret will also be mounted inside all of the Pod's containers at <code>/mnt/secret-vol0</code> path exposing <code>key</code> as file on <code>/data/configmap/key</code>.</p> <p>Warning</p> <p>Secret <code>type</code> parametar can be any of the Kubernetes Types of Secret. However, it is up to the end user to define required keys in the <code>data</code> parametar for the Secret type being deployed.</p> <p>If <code>type</code> is omitted, Secret type will default to <code>Opaque</code></p> <p>Tip</p> <p>Common usecase in the above scenario would be creating and automatically mounting any configuration files or secretes your application might need during its runtime.</p> <p>See also:</p> <ul> <li>configMaps</li> </ul>"},{"location":"start/values/extravolumemounts.html","title":"extraVolumeMounts","text":"<p>Feature state:  1.1.0</p> <p><code>extraVolumeMounts</code> value in Kubedeploy allow for defining additional mounted volumes for all containers in a Pod.</p> <p>Note</p> <p>By default <code>extraVolumeMounts</code> in Kubedeploy will not mount any additional volumes.</p> <p>Allowed volume mounts include:</p> <ul> <li><code>existingClaim</code> - this can be used to mount any chart external persistent volume claims in any type of deploymentMode. By chart's design only statefulset will have this ability</li> <li> <p><code>hostPath</code> - can be used to mount any filesystem directories directly from Kubernetes node in Pod</p> <p>Warning</p> <p>HostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the required file or directory, and mounted as ReadOnly.</p> </li> <li> <p><code>csi</code> - allows mounting any volumes exposed with csi drivers, for example secrets-store-csi-driver.</p> </li> <li><code>emptyDir</code> - temporary empty directories for Pod's container to share (see emptyDir for more info).</li> <li><code>secretName</code> - allows mounting any Secret objects. If you define secrets in chart you can also use extraSecrets mount ability.</li> <li><code>configMapName</code> - allows mounting any configMap objects. If you define configmaps in chart you can also use configMaps mount ability.</li> </ul> <p>secret and configmap extra mounts</p> <p>extraSecret and configMaps values section will mount entire objects as directories in containers. This might not be deisred when you wish to do items remapping or mount just a single file via subPath. extraVolumeMounts allows for such scenarios.</p> <p><code>secretName</code> and <code>configMapName</code> parameter in extraVolumeMounts will pass names as is by default. This allows for chart external objects to be referenced. If you are referencing the Secrets or ConfigMaps deployed with this chart consider using <code>chartName: true</code> option. <code>chartName</code> option will prefix provided name with Chart's <code>fullname</code> template, otherwise it is up to the user to define full name of the Secret and ConfigMap object as seen in the K8s cluster.</p> <p>Define extraVolumeMounts</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nextraVolumeMounts:\n- name: extra-volume-0\nmountPath: /mnt/volume0\nreadOnly: true\nexistingClaim: volume-claim\n- name: extra-volume-1\nmountPath: /mnt/volume1\nreadOnly: true\nhostPath: /usr/shared/\n- name: external-secrets\nmountPath: /mnt/volume2\ncsi: true\ndata:\ndriver: secrets-store.csi.k8s.io\nreadOnly: true\nvolumeAttributes:\nsecretProviderClass: \"secret-provider-name\"\n- name: empty-dir-vol\nmountPath: /mnt/volume3\n- name: secret-mount\nmountPath: /mnt/volume4\nsecretName: my-secret\noptional: true  # If set to true, kubernetes will ignore this mount if secret is not available\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>As a result of the above example, Kubdeploy will mount five additional volumes to all containers in Pod.</p> <ol> <li>existing volume claim with name <code>volume-claim</code> will be mounted at <code>/mnt/volume0</code></li> <li><code>/usr/shared/</code> from Kubernetes node will be mounted at <code>/mnt/volume1</code></li> <li><code>csi</code> secret would be mounted at <code>/mnt/volume2</code></li> <li><code>emptyDir</code> will be mounted at <code>/mnt/volume3</code></li> <li><code>secret</code> with name <code>my-secret</code> will be mounted at <code>/mnt/volume4</code></li> </ol> <p>Define extraVolumeMounts with subPath and chart defined Secret / Configmap</p> <p>Feature state:  1.2.2</p> <p>While this example is not something that is recommended for production, it will serve its purpose for showcasing several features in play at once.</p> <p>We will deploy single configmap with two keys holding configuration data for nginx. <code>nginx.conf</code> for main config, and second <code>my-vhost.conf</code> for vhost config.</p> <p>Then we use <code>extraVolumeMounts</code> to mount <code>nginx.conf</code> as subpath on: <code>/etc/nginx/nginx.conf</code> which will mount only the first key preserving all other directory data in <code>/etc/nginx</code>.</p> <p>Since we include entire /etc/nginx/conf.d/*.conf files we can't simply mount entire configmap to that folder. First ConfigMap key would be picked up again and brake the server configuration. By using <code>items</code> we can remap the first config key <code>nginx.conf</code> to <code>disabled-conf</code> and then mount the entire configmap at <code>/etc/nginx/conf.d</code> path.</p> <p>Tip</p> <p>In normal scenario you would create two separate configmaps for vhost data and one for main config. Then you can skip the whole remapping part with items and end up with cleaner config. As mentioned, this example is merely here for demonstration purposes.</p> <p>When using subPath, file updates will not be automatically reflected in the pods. They need to be restarted to pick up changes. To accomplish that we define <code>configMapHash: true</code> so that the chart will generate new replicaset for deployment whenever <code>configMaps</code> is updated.</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nconfigMaps:\n- name: my-nginx-config\ndata:\nnginx.conf: |\n... config here ...\ninclude /etc/nginx/conf.d/*.conf\n...\nmy-vhost.conf: |\n... vhost config here ...\nconfigMapsHash: true\nextraVolumeMounts:\n- name: nginx-conf\nconfigMapName: my-nginx-config\nchartName: true\nmountPath: /etc/nginx/nginx.conf\nsubPath: nginx.conf\n- name: nginx-conf-d\nconfigMapName: my-nginx-config\nchartName: true\nmountPath: /etc/nginx/conf.d\nitems:\n- key: nginx.conf\npath: disabled-conf\n- key: my-vhost.conf\npath: 00_default.conf\noptional: true  # If set to true, kubernetes will ignore this mount if secret is not available\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the end we will end up with following structure in our pod:</p> <pre><code>/etc\n`-- nginx\n    |-- conf.d\n    |   |-- 00_default.conf -&gt; ..data/00_default.conf\n    |   `-- disabled-conf -&gt; ..data/disabled-conf\n    `-- nginx.conf\n</code></pre> <p>While the above example is using ConfigMaps, if you have sensitive data, same can be done with Secrets.</p> <p>See also:</p> <ul> <li>configMaps</li> <li>configMapsHash</li> <li>extraSecrets</li> </ul>"},{"location":"start/values/fullnameoverride.html","title":"fullnameOverride","text":"<p>Kubedeploy generates object names in templates based on <code>kubedeploy.fullname</code> helper template. By default all objects will have names calculated from Helm <code>release name</code> suffixed by <code>kubedeploy</code>.</p> <p>Default name for Deployment</p> values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Deployment\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Will result in deployment name: <code>webserver-kubedeploy</code></p> <p><code>fullnameOverride</code> value can be used to change the default deployed resource names by changing the full deployment name.</p> <p>When to use fullnameOverride?</p> <p>Kubedeploy is a generic chart, as such its chart name does not reflect on application being deployed. <code>fullnameOverride</code> should be used when you don't plan to install multiple instances of your application, and you want a shorter deployment names.</p> <p>fullnameOverride for Deployment</p> values.yaml<pre><code>fullnameOverride: nginx\nimage:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Deployment\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Will result in deployment name: <code>nginx</code></p> <p>See also: nameOverride</p>"},{"location":"start/values/healthcheck.html","title":"healthcheck","text":"<p>Feature state:  0.6.0</p> <p><code>healthcheck</code> value in Kubedeploy can be used to define custom liveness, readiness and startup Probes for main container.</p> <p>Note</p> <p>By default <code>healthcheck</code> in Kubedeploy is configured with auto-detect. If <code>ports</code> have defined a port named <code>http</code>, Kubedeploy will automatically define httpCheck liveness and readiness probes on base URL.</p> automatic probes on main container<pre><code>      livenessProbe:\nhttpGet:\npath: /\nport: http\nreadinessProbe:\nhttpGet:\npath: /\nport: http\n</code></pre> <p>When defining custom <code>healthcheck</code> probes any automatic probes will be disabled.</p> <p>Define custom healthcheck probes</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nports:\n- name: liveness-port\ncontainerPort: 8080\nprotocol: TCP\nhealthcheck:\nenabled: true\nprobes:\nlivenessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 5\nperiodSeconds: 10\nreadinessProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 1\nperiodSeconds: 10\nstartupProbe:\nhttpGet:\npath: /healthz\nport: liveness-port\nfailureThreshold: 30\nperiodSeconds: 10\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>If you wish to disable automatic probes without configuring custom probes (Feature state:  0.8.2):</p> <p>Disable automatic healthcheck probes</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nhealthcheck:\ndisableAutomatic: true\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Note</p> <p>For more complete example of configuring healthcheck probes with other configurable values, please see the Best practices linked page.</p> <p>See also:</p> <ul> <li>ports</li> <li>Best practices</li> </ul>"},{"location":"start/values/image.html","title":"image","text":"<p><code>image</code> value in Kubedeploy controls the main container image settings.</p> <p>If left unchanged these are the default values that Kubedeploy will work with:</p> Default image configuration<pre><code>image:\nrepository: nginx # (1)\npullPolicy: IfNotPresent # (2)\ntag: \"latest\" # (3)\ncommand: [] # (4)\nargs: [] # (5)\nlifecycle: {} # (6)\nterminationGracePeriodSeconds: 30 # (7)\n</code></pre> <ol> <li>Defines container repository</li> <li>Default container pull policy</li> <li>Defines container image tag</li> <li>Defines container custom command. Reference</li> <li>Define container custom arguments. Reference</li> <li>(Feature state:  1.0.0) Define container custom lifecycle hooks. More info</li> <li>Define Pod terminationGracePeriodSeconds.</li> </ol> <p>If we wish to deploy different webserver we can do this by simply changing the image repository.</p> <p>Apache webserver</p> values.yaml<pre><code>image:\nrepository: httpd\ntag: 2.4\npullPolicy: Always\nreplicaCount: 3\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Configurable container lifecycle</p> <p>Main container is intended to run main application inside a Pod. You can augment it with <code>additionalContainers</code> and <code>initContainers</code>. Only main container supports defining custom lifecycle hooks. See the related documentation from Best practices on use cases.</p> <p>See also:</p> <ul> <li>lifecycle hooks</li> <li>initContainers</li> <li>additionalContainers</li> </ul>"},{"location":"start/values/imagepullsecrets.html","title":"imagePullSecretes","text":"<p><code>imagePullSecrets</code> value in Kubedeploy should contain a list of Secret objects that contains necessary configuration for pulling container images from private registries.</p> <p>See the official documentation on this feature.</p> <p>By default no <code>imagePullSecretes</code> are defined in Kubedeploy.</p> <p>Define extraSecrets and imagePullSecrets</p> <p>We will use Kubedeploys extraSecrets value for defining Secret object later used in <code>imagePullSecretes</code>.</p> values.yaml<pre><code>nameOverride: my-app # (1)\nimage:\nrepository: private-repo/image-name\ntag: latest\npullPolicy: Always\nimagePullSecrets:\n- webapp-my-app-imagepullsecret # (2)\nextraSecrets:\n- name: imagepullsecret # (3)\ntype: kubernetes.io/dockerconfigjson\ndata:\n.dockerconfigjson: |\ncontent of ~/.docker/config.json file # (4)\n</code></pre> <ol> <li>define nameOverride for easier secret reference</li> <li>secret name deployed with extraSecrets is prefixed with <code>kubedeploy.fullname</code> templated.</li> <li>create extra secret named imagepullsecret</li> <li>content should be replaced with content from <code>~/.docker/config.json</code> file</li> </ol> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Note</p> <p>Secret objects deployed with extraSecrets are prefixed with deployment full name.</p> <p>Tip</p> <p>When deploying multiple applications from same private image registry, it is recommended to create Secret object by hand and then reference it in Kubedeploy.</p> <p>Example</p> <p>Create a secret</p> Create my-registry-creds secret<pre><code>kubectl create secret docker-registry my-registry-creds \\\n--docker-email=tiger@acme.example \\\n--docker-username=tiger \\\n--docker-password=pass1234 \\\n--docker-server=my-registry.example:5000\n</code></pre> <p>Reference the manually created secret</p> values.yaml<pre><code>image:\nrepository: my-registry.example/image-name\ntag: latest\npullPolicy: Always\nimagePullSecrets:\n- my-registry-creds\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>See also:</p> <ul> <li>extraSecretes</li> <li>nameOverride</li> </ul>"},{"location":"start/values/ingress.html","title":"ingress","text":"<p>Ingress is an API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress may provide load balancing, SSL termination and name-based virtual hosting. In short, Ingress is a way to map your application to publicly available domain name.</p> <p><code>ingress</code> value in Kubedeploy allows you to control parameters for Ingress object that would be created by the chart.</p> <p>Note</p> <p>By default <code>ingress</code> is disabled.</p> <p>Available values for <code>ingress</code> in Kubedeploy:</p> <pre><code>ingress:\nenabled: false # (1)\nclassName: \"haproxy\" # (2)\npathType: ImplementationSpecific # (3)\nwithSSL: true # (4)\nsvcPort: \"\" # (5)\nannotations: # (6)\ncert-manager.io/cluster-issuer: letsencrypt\nhosts: # (7)\n- host: \"\" # (8)\npaths: [] # (9)\ntls: [] # (10)\n</code></pre> <ol> <li>Enable Ingres object</li> <li>Ingress class name. In case you have multiple Ingress controllers you can specifically target desired one. Defaults to \"haproxy\"</li> <li>Default Ingress pathType</li> <li>(Feature state:  1.1.0) Deploy Ingress object with SSL support. Automatically configures the Ingress <code>tls</code> spec.</li> <li>(Feature state:  1.1.0) Define default Service port that will be targeted by Ingress. If left undefined Ingress will use first port from <code>service.ports</code> (or first port from <code>ports</code>) to route traffic to.</li> <li>Additional Ingress annotations, can be used to define custom Ingress controller configuration. By default, it sets the cert-manager annotation to use <code>letsencrypt</code> cluster-issuer for issuing SSL certificates for configured ingress domain.</li> <li>(list) Ingress host list.</li> <li>(string, required) Define Ingress hostname</li> <li> <p>(list, optional) Ingress host paths.     example usage:     (Feature state:  1.1.0)</p> <p>Example</p> <pre><code>ingress:\nenabled: true\nhosts:\n- host: \"my-domain.com\"\npaths:\n- path: /api # (optional) defaults to /\nsvcPort: 8080 # (optional) defaults to `ingress.svcPort`\npathType: Prefix # (optional) defaults to `ingress.pathType`\n</code></pre> </li> <li> <p>(list, optional) Ingress TLS list.     overrides any auto configured tls config created by withSSL.</p> <p>Allows for custom secretName and host list to be defined. In case when you have pre-configured SSL stored in Kubernetes Secret. If secret does not exist, new one will be created by cert-manager.</p> <p>example usage:</p> <p>Example</p> <pre><code>ingress:\nenabled: true\nhosts:\n- host: \"my-domain.com\"\ntls:\n- hosts: # (required) list of hosts covered by ssl in secret\n- my-domain.com\nsecretName: my-domain-ssl # name of secret where SSL is configured\n</code></pre> </li> </ol> <p><code>ingress</code> values might look overwhelming at first. However, it should be fairly simple to expose your app on custom domain.</p> <p>Simple ingress object</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\nports:\n- containerPort: 80\nname: http\ningress:\nenabled: true\nhosts:\n- host: \"my-domain.com\"\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>As a result of the above configuration, Ingress object is created using <code>haproxy</code> ingressClass. <code>tls</code> section is automatically configured enabling SSL coverage for your domain.  <code>letsencrypt</code> cluster issuer provided by cert-manager will be used to issue valid SSL certificate. And finally, all traffic for <code>my-domain.com</code> is routed to our Service objects port 80.</p> <p>A more complex example, would be application with frontend and backend service ports. For that we will use additional containers. Our additional container will expose static frontend for our website, and our main container will expose backend API that will respond on /api path.</p> <p>We will also configure our <code>ingress</code> to use custom SSL certificate installed as <code>extraSecret</code>.</p> <p>Complex ingress object</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\n# define ports for main container (backend)\nports:\n- containerPort: 80\nname: backend\n- containerPort: 8080\nname: metrics\n# define additional container for frontend\nadditionalContainers:\nenabled: true\ncontainers:\n- name: my-app-frontend\nrepository: my-app-frontend\ntag: latest\n# expose frontend ports\nports:\n- containerPort: 9000\nname: frontend\nprotocol: TCP\n# create extra secret containing SSL certificate for my-domain.com\nextraSecrets:\n- name: my-domain-ssl\ntype: kubernetes.io/tls\ndata:\ntls.crt: |\n--------BEGIN CERTIFICATE-----\nMIIC2DCCAcCgAwIBAgIBATANBgkqh ...\ntls.key: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...\ntls.ca: |\n--------BEGIN CERTIFICATE-----\nMIIC2DCCAcCgAwIBAgIBATANBgkqh ...\n# create custom service object exposing both frontend and backend ports\nservice:\nports:\n- port: 9000\ntargetPort: frontend\nprotocol: TCP\nname: frontend\n- port: 80\ntargetPort: backend\nprotocol: TCP\nname: backend\n# our custom ingress\ningress:\nenabled: true\nhosts:\n- host: \"my-domain.com\"\npaths:\n- path: /\nsvcPort: 9000 # set explicitly (first port is default anyway)\n- path: /api\nsvcPort: 80 # target backend port for /api urls\ntls:\n- hosts:\n- my-domain.com\n# use secret provisioned with extraSecrets\n# remember: name is prefixed with fullname\nsecretName: webap-my-app-my-domain-ssl\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>If you require more than one Ingress object per deployment, for example targeting different ingressClasses, please see extraIngress configuration options</p> <p>See also:</p> <ul> <li>extraIngress</li> <li>ports</li> <li>service</li> <li>additionalContainers</li> <li>extraSecrets</li> </ul>"},{"location":"start/values/initcontainers.html","title":"initContainers","text":"<p>Feature state:  0.5.0</p> <p><code>initContainers</code> value in Kubedeploy allow for defining one or more initContainers.</p> <p>Init containers are specialized containers that run before app containers in a Pod. Init containers can contain utilities or setup scripts not present in an app image.</p> <p>A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.</p> <p>Init containers are exactly like regular containers, except:</p> <ul> <li>Init containers always run to completion.</li> <li>Each init container must complete successfully before the next one starts.</li> </ul> <p>If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. However, if the Pod has a restartPolicy of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.</p> <p>If you specify multiple init containers for a Pod, kubelet runs each init container sequentially. Each init container must succeed before the next can run. When all of the init containers have run to completion, kubelet initializes the application containers for the Pod and runs them as usual.</p> <p>Note</p> <p>By default, Kubedeploy does not specify any <code>initContainers</code></p> <p>Supported <code>initContainers</code> values in Kubedeploy (click on the plus sign for more info):</p> <pre><code>initContainers:\nenabled: false # (1)\npullPolicy: IfNotPresent # (2)\nresources: {} # (3)\ncontainers: # (4)\n- name: \"busybox-init\" # (5)\nrepository: busybox # (6)\ntag: \"latest\" #(7)\ncommand: [\"sh\", \"-c\", \"exit 0\"] # (8)\nargs: [] # (9)\nresources: {} # (10)\n</code></pre> <ol> <li>Define if initContainers are enabled.</li> <li>initContainers image pull policy</li> <li> <p>(Feature state:  1.0.0) (optional) Define initContainers global resource requests and limits. Will be applied to all initContainers if more specific (per container) resource requests and limits are not defined.</p> <p>Example</p> <pre><code>initContainers:\nresources:\nrequests:\ncpu: 0.1\nmemory: 128Mi\nlimits:\ncpu: 0.2\nmemory: 256Mi\n</code></pre> </li> <li> <p>(list) Sequential list of initContainers</p> </li> <li>(required) Define initContainer name</li> <li>(required) Define initContainer repository</li> <li>(optional) Define initContainer image tag, defaults to <code>latest</code> if unspecified</li> <li>(optional) Define custom command for initContainer</li> <li>(optional) Define custom arguments for initContainer</li> <li>(Feature state:  1.0.0) (optional) Define custom resources for this specific initContainer</li> </ol> <p>Init containers can be used to delay application startup until external resource is ready, initialize the application, or run database migrations if nececary.</p> <p>Shared values/volumes</p> <p><code>initContainers</code> will automatically inherit all the environment values set in <code>env</code>, <code>envFrom</code> and volume mounts defined in <code>configMaps</code>, <code>extraSecrets</code> and <code>extraVoumeMounts</code> just as the main container would.</p> <p>Define initContainers that waits for mydb service</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\ninitContainers:\nenabled: true\nresources:\nrequests:\ncpu: 0.1\nmemory: 128Mi\nlimits:\ncpu: 0.2\nmemory: 256Mi\ncontainers:\n- name: wait-for-db\nrepository: busybox\ncommand:\n- sh\n- -c\n- \"until nslookup mydb; do echo waiting for mydb; sleep 2; done\"\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>See also:</p> <ul> <li>additionalContainers</li> <li>env</li> <li>envFrom</li> <li>configMaps</li> <li>extraSecrets</li> <li>extraVolumeMouts</li> </ul>"},{"location":"start/values/jobspec.html","title":"jobspec","text":"<p><code>jobspec</code> value in Kubedeploy defines Jobs specific parameters.</p> <p>Note</p> <p><code>jobspec</code> will be only taken into account if the <code>deploymetMode</code> is set to <code>Job</code>.</p> <p>Available values for <code>jobspec</code> in Kubedeploy:</p> <pre><code>jobspec:\nrestartPolicy: OnFailure # (1)\ncommand: [] # (2)\nargs: [] # (3)\nparallelism: 1 # (4)\nbackoffLimit: 3 # (5)\nttlSecondsAfterFinished: 300 # (6)\n</code></pre> <ol> <li>Define restart policy for jobs if deploymentMode=Job, see reference</li> <li>Define command for Job</li> <li>Define args for Job</li> <li>Define Job paralelisam, see reference</li> <li>Define Job backoff limit, see reference</li> <li>(Feature state:  1.2.0) Define Automatic Cleanup for Finished Jobs</li> </ol> <p>Deprecation Warning</p> <p>Starting from Kubedeploy version 1.2, you should begin using image.command and image.args instead of cronjobspec.command and cronjobspec.args. These values will remain available as failsafe options until Kubedeploy 2.0, at which point they will be removed.</p> <p>Define job</p> values.yaml<pre><code>nameOverride: my-job\ndeploymentMode: Job\nimage:\nrepository: busybox\ntag: latest\njobspec:\ncommand: [\"sh\", \"-c\", \"echo hello from job\" ]\n</code></pre> Deploy command<pre><code>helm install hello sysbee/kubedeploy -f values.yaml\n</code></pre> <p>The above example will create a Job that will run once and echo a hello message.</p> <p>See also:</p> <ul> <li>deploymentMode</li> <li>cronjobspec</li> </ul>"},{"location":"start/values/keda.html","title":"keda","text":"<p>Feature state:  0.9.0</p> <p><code>keda</code> value in Kubedeploy allows controlling the parameters for Kubernetes Event-driven Autoscaling: KEDA 2.x</p> <p>Note</p> <p>To use <code>keda</code>, your cluster must have KEDA operator installed first. If CRD is not detected, chart will skip rendering keda maifests.</p> <p>Warning</p> <p><code>keda</code> and <code>autoscaler</code> can't be used at the same time</p> <p>Available values for <code>keda</code> in Kubedeploy:</p> <pre><code>keda:\nenabled: false # (1)\nminReplicas: 1 # (2)\nmaxReplicas: 10 # (3)\npollingInterval: 30 # (4)\ncooldownPeriod: 300 # (5)\nrestoreToOriginalReplicaCount: false # (6)\nscaledObject:\nannotations: {} # (7)\nbehavior: {} # (8)\ntriggers: [] # (9)\n</code></pre> <ol> <li>enables keda, Note: mutually exclusive with HPA, enabling KEDA disables HPA</li> <li>Number of minimum replicas for KEDA autoscaling</li> <li>Number of maximum replicas for KEDA autoscaling</li> <li>Interval for checking each trigger ref</li> <li>The period to wait after the last trigger reported active before scaling the resource back to 0 ref</li> <li>After scaled object is deleted return workload to initial replica count ref</li> <li>Scaled object annotations, can be used to pause scaling ref</li> <li>HPA configurable scaling behavior see ref</li> <li>Keda triggers ref</li> </ol> <p>Keda allows for more precise triggers on when the application will be scaled out. It does require external metrics system and access to it. In the example below we will configure haproxy application and keda scaling triggers based on application provided metrics collected by Prometheus.</p> <p>Define keda scaling triggers</p> values.yaml<pre><code>nameOverride: my-lb\nimage:\nrepository: haproxy\ntag: latest\nports:\n- contaierPort: 80\nname: http\n- containerPort: 443\nname: https\n- containerPort: 1024\nname: metrics\nmonitoring:\nenabled: true\nkeda:\nenabled: true\nminReplicas: 2\nmaxReplicas: 10\ntriggers:\n- type: prometheus\nmetadata:\nserverAddress: http://&lt;prometheus-host&gt;:9090\nmetricName: haproxy_process_idle_time_percent\nthreshold: '50'\nquery: avg(100-avg_over_time(haproxy_process_idle_time_percent{container=\"kubernetes-ingress-controller\",service=\"mytest-kubernetes-ingress\"}[2m]))\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>See also:</p> <ul> <li>resources</li> <li>autoscaling</li> </ul>"},{"location":"start/values/kubeversionoverride.html","title":"kubeVersionOverride","text":"<p>When generating Kubernetes manifest files for objects, Kubedeploy will sometimes check the Kubernets API for its version to determine API compatibility.</p> <p><code>kubeVersionOverride</code> value can be used to change the automatically deteceted Kubernetes version.</p> <p>When to use kubeVersionOverride?</p> <p>If you encounter compatibility issues, or if you are generating templates without the active Kubernetes cluster.</p> <p>kubeVersionOverride</p> values.yaml<pre><code>kubeVersionOverride: \"1-22.0-0\"\nimage:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Deployment\n</code></pre>"},{"location":"start/values/minreadyseconds.html","title":"minReadySeconds","text":"<p>Feature state:  1.0.0</p> <p><code>minReadySeconds</code> value in Kubedeploy can be used to define custom minReadySeconds for deployments and statefulsets.</p> <p>Note</p> <p>By default <code>minReadySeconds</code> in Kubedeploy is set to 10 seconds.</p> <p>custom minReadySeconds</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nminReadySeconds: 60\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Note</p> <p>For more complete example of configuring minReadySeconds with other configurable values, please see the Best practices linked page.</p> <p>See also:</p> <ul> <li>Best practices</li> </ul>"},{"location":"start/values/monitoring.html","title":"monitoring","text":"<p>Feature state:  0.7.0</p> <p><code>monitoring</code> value in Kubedeploy allows controlling the parameters for deploying ServiceMonitor or PodMonitor objects.</p> <p>Note</p> <p>ServiceMonitor and PodMonitor objects are part of CRD definitions of Prometheus operator. Your cluster must have them installed first. If CRD is detected as missing, chart will skip their deployment.</p> <p>Available values for <code>monitoring</code> in Kubedeploy:</p> <pre><code>monitoring:\nenabled: false # (1)\nlabels: {} # (2)\ntargetLabels: [] # (3)\nscrapePort: metrics # (4)\nscrapePath: /metrics # (5)\nscrapeInterval: 20s # (6)\nscrapeTimeout: 15s # (7)\nscheme: http # (8)\ntlsConfig: {} # (9)\nmetricRelabelings: [] # (10)\nrelabelings: [] # (11)\n</code></pre> <ol> <li>Enable monitoring.</li> <li>Provide additional labels to the ServiceMonitor metadata</li> <li>Additional metric labels</li> <li>Provide named service port used for scraping</li> <li>Provide HTTP path to scrape for metrics.</li> <li>Provide interval at which metrics should be scraped</li> <li>Timeout after which the scrape is ended (must be less than scrapeInterval)</li> <li>HTTP scheme to use for scraping.</li> <li>TLS configuration to use when scraping the endpoint</li> <li>Provide additional metricRelabelings to apply to samples before ingestion.</li> <li>Provide additional relabelings to apply to samples before scraping</li> </ol> <p>Info</p> <p>If <code>service.enabled</code> is <code>True</code>, chart will generate <code>ServiceMonitor</code> object, otherwise <code>PodMonitor</code> will be used.</p> <p>Enable metrics scraping for exporter container</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nports:\n- containerPort: 80\nname: http\nprotocol: TCP\nadditionalContainers:\nenabled: true\ncontainers:\n- name: metrics-exporter\nrepository: nginx/nginx-prometheus-exporter\ntag: latest\nargs:\n- -nginx.scrape-uri=http://localhost:80/stub_status\nports:\n- containerPort: 9113\nname: metrics\nprotocol: TCP\nmonitoring:\nenabled: true\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In this simple example our additionalContainer is used as metrics exporter. Since it already exports metrics on port named metrics, all we need to do is enable monitoring and leave all the other values on default.</p> <p>Enable metrics scraping for built-in application metrics</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nports:\n- containerPort: 80\nname: http\nprotocol: TCP\nmonitoring:\nenabled: true\nscrapePort: http\nscrapePath: /custom/metrics/url\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>If our application has built in metrics exporter on custom route, we can modify monitoring <code>scrapePort</code> and <code>scrapePath</code>.</p> <p>See also:</p> <ul> <li>ports</li> <li>service</li> <li>additionalContainers</li> </ul>"},{"location":"start/values/nameoverride.html","title":"nameOverride","text":"<p>Kubedeploy generates object names in templates based on <code>kubedeploy.fullname</code> helper template. By default all objects will have names calculated from Helm <code>release name</code> suffixed by <code>kubedeploy</code>.</p> <p>Default name for Deployment</p> values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Deployment\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Will result in deployment name: <code>webserver-kubedeploy</code></p> <p><code>nameOverride</code> value can be used to change the default deployed resource names by changing the chart name as suffix.</p> <p>When to use nameOverride?</p> <p>Kubedeploy is a generic chart, as such its chart name does not reflect on application being deployed. If you need to deploy multiple instances of your applciation it is recommended to set the <code>nameOverride</code> value to your application name, and assign the <code>release name</code> to your specific installation instance.</p> <p>nameOverride for Deployment</p> values.yaml<pre><code>nameOverride: nginx\nimage:\nrepository: nginx\ntag: 1.25.2\ndeploymentMode: Deployment\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Will result in deployment name: <code>webserver-nginx</code></p> <p>shortening the deployed application name</p> <p>In cases where <code>nameOverride</code> is equal to <code>release name</code>, Kubedeploy will shorten the deplyed resource names to just <code>release name</code>.</p> <p>See also: fullnameOverride</p>"},{"location":"start/values/networkpolicy.html","title":"networkPolicy","text":"<p>Feature state:  1.0.0</p> <p><code>networkPolicy</code> value enables defining of raw NetworkPolicy rules.</p> <p>In addition, Kubedeploy will create automatic Network Policy rules when <code>ingress</code> or <code>monitoring</code> values are enabled.</p> <p>Note</p> <p>By default <code>networkPolicy</code> rules are disabled.</p> <p>Default values for <code>networkPolicy</code> values are:</p> <pre><code>networkPolicy:\nenabled: false # (1)\ningressNamespace: ingress # (2)\nmonitoringNamespace: monitoring # (3)\ningress: [] # (4)\negress: [] # (5)\n</code></pre> <ol> <li> <p>Enables Pod based NetworkPolicy</p> </li> <li> <p>Define Namespace where Ingress controller is deployed.     Used to generate automatic policy to enable ingress access when .Values.ingress is enabled</p> </li> <li>Define namespace where monitoring stack is deployed     Used to generate automatic policy to enable monitoring access when .Values.monitoring is enabled</li> <li>(list) Define spec.ingress for NetowkPolicy rules</li> <li>(list) Define spec.egress for NetowkPolicy rules</li> </ol> <p>Warning</p> <p>Enabling networkPolicy without defining any ingres or egress rules will result in cutting off all in/out traffic to the pod. If your Application needs access to or from external applications/services, please make sure you define proper rules in ingress and egress configuration lists.</p> <p>Info</p> <p>Automatic rules will always define allow rules from <code>ingressNamespace</code> if <code>ingress.enabled</code> is set to <code>true</code>. Same goes for <code>monitoringNamespace</code> if <code>monitoring.enabled</code> is set to <code>true</code>.</p> <p>Define custom networkPolicy</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\ningres:\nenabled: true\nhosts:\n- host: mydomain.com\nnetworkPolicy:\nenabled: true\ningressNamespace: ingress\negress:\n- to:\n- ipBlock:\ncidr: 0.0.0.0/0\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example we define ingress object, and enable only <code>egress</code> rules in <code>newtorkPolicy</code>.</p> <p>As a result, Pod will be allowed all outbound network traffic. However, inbound traffic to the Pod will only be allowed from <code>ingressNamespace</code>.</p> <p>See also:</p> <ul> <li>ingress</li> <li>monitoring</li> </ul>"},{"location":"start/values/nodeselector.html","title":"nodeSelector","text":"<p>Each node in the cluster is assigned a predefined set of common labels. Node labels can be utilized in Pod scheduling decisions by defining node selectors.</p> <p>Kubedeploy offers this functionality via the <code>nodeSelector</code> value config option, enabling you to specifically target a particular node or group of nodes.</p> <p>Note</p> <p>By default <code>nodeSelector</code> is undefined.</p> <p>Define custom nodeSelector</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nnodeSelector:\nkarpenter.sh/capacity-type: on-demand\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example, Kubernetes scheduler will only consider nodes containing the <code>karpenter.sh/capacity-type: on-demand</code> label. If no nodes in the cluster have that label Pod will be stuck in <code>Pending</code> state.</p> <p>Tip</p> <p>When using cluster automatic scaling software like <code>Karpenter</code> or <code>Cluster autoscaler</code>, node selectors can be used to influence which types of nodes will be added to the cluster. Please see Best pracices linked below for more info.</p> <p>See also:</p> <ul> <li>podAntiAffinity</li> <li>podAntiAffinityTopologyKey</li> <li>affinity</li> <li>topologySpreadConstraints</li> <li>Best practices</li> </ul>"},{"location":"start/values/persistency.html","title":"persistency","text":"<p><code>persistency</code> value in Kubedeploy defines StatefulSets persistent volume claims parameters.</p> <p>Note</p> <p><code>persistency</code> will be only taken into account if the <code>deploymetMode</code> is set to <code>Statefulset</code>.</p> <p>Available values for <code>persistency</code> in Kubedeploy:</p> <pre><code>persistency:\nstorageClassName: # (1)\nenabled: false # (2)\ncapacity:\nstorage: 5Gi # (3)\naccessModes: # (4)\n- ReadWriteOnce\nmountPath: \"/data\" # (5)\n</code></pre> <ol> <li>(string) Define custom name for persistent storage class name     @default -- uses cluster default storageClassName</li> <li>Enable support for persistent volumes.     Supported only if deploymentMode=Statefulset.</li> <li>Define storage capacity</li> <li>Define storage access modes. Must be supported by available storageClass</li> <li>Define where persistent volume will be mounted in containers.</li> </ol> <p>Define persistent volume for statefulsets</p> values.yaml<pre><code>nameOverride: my-app\ndeploymentMode: Statefulset\nimage:\nrepository: nginx\ntag: 1.25.2\npersistency:\nenabled: true\ncapacity:\nstorage: 20Gi\nmountPath: /var/www/data\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>The above example will create a StatefulSet with 20Gi persistent volume mounted at <code>/var/www/data</code>.</p> <p>See also:</p> <ul> <li>deploymentMode</li> </ul>"},{"location":"start/values/podannotations.html","title":"podAnnotations","text":"<p><code>podAnnotations</code> value in Kubedeploy can be used to define additional Pod annotations.</p> <p>Note</p> <p>By default no extra <code>podAnnotations</code> are defined.</p> <p>Define extra podAnnotations</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\npodAnnotations:\nkarpenter.sh/do-not-evict: \"true\"\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>If your cluster is using Karpenter for cluster scalig, with the above example You can block Karpenter from voluntarily choosing to disrupt certain pods by setting the <code>karpenter.sh/do-not-evict: \"true\"</code> annotation on the pod.</p> <p>This is useful for pods that you want to run from start to finish without disruption. By opting pods out of this disruption, you are telling Karpenter that it should not voluntarily remove a node containing this pod.</p> <p>See also:</p> <ul> <li>podExtraLabels</li> </ul>"},{"location":"start/values/podantiaffinity.html","title":"podAntiAffinity","text":"<p>Feature state:  1.0.0</p> <p><code>podAntiAffinity</code> can prevent the scheduler from placing application replicas on the same node.</p> <p>The value <code>soft</code> means that the scheduler should prefer to not schedule two replica pods onto the same node but no guarantee is provided.</p> <p>The value <code>hard</code> means that the scheduler is required to not schedule two replica pods onto the same node.</p> <p>The default value <code>\"\"</code> will disable pod anti-affinity so that no anti-affinity rules will be configured.</p> <p>Note</p> <p>By default <code>podAntiAffinity</code> is undefined.</p> <p>Define podAntiAffinity</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\npodAntiAffinity: \"soft\"\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example three replicas of application would prefer to be scheduled on different Kubernetes nodes.</p> <p>For more examples please see the Best practices linked below.</p> <p>See also:</p> <ul> <li>podAntiAffinityTopologyKey</li> <li>affinity</li> <li>nodeSelector</li> <li>topologySpreadConstraints</li> <li>Best practices</li> </ul>"},{"location":"start/values/podantiaffinitytopologykey.html","title":"podAntiAffinityTopologyKey","text":"<p>Feature state:  1.0.0</p> <p><code>podAntiAffinityToplologyKey</code> sets the TopologyKey to use for automatic <code>podAntiAffinity</code> rules.</p> <p>Note</p> <p>By default <code>podAntiAffinityTopologyKey</code> is set to <code>kubernetes.io/hostname</code>.</p> <p>Define podAntiAffinity with zone toplogy key</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\npodAntiAffinity: \"soft\"\npodAntiAffinityTopologyKey: kubernetes.io/zone\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example three replicas of application would prefer to be scheduled on Kubernetes nodes spread in different availability zones.</p> <p>For more examples please see the Best practices linked below.</p> <p>See also:</p> <ul> <li>podAntiAffinity</li> <li>affinity</li> <li>nodeSelector</li> <li>topologySpreadConstraints</li> <li>Best practices</li> </ul>"},{"location":"start/values/poddisruptionbudget.html","title":"podDisruptionBudget","text":"<p>Pods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system software error.</p> <p>See Best Practices for more information on voluntary and involuntary disruptions.</p> <p><code>podDisruptionBudget</code> value in Kubedeploy allows you to define budgets for voluntary disruptions.</p> <p>Note</p> <p>By default <code>podDisruptionBudget</code> is undefined.</p> <p>Create podDisruptionBudget with multiple replicas</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\npodDisruptionBudget:\nenabled: true\nminAvailable: 2\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the example configuration above, we ensure that our Pod has 3 replicas at normal runtime. Configured PodDisruptionBudget requires that at least 2 replicas are always available during any voluntary disruptions.</p> <p>Info</p> <p>To learn more about Pod disruptions and PodDisruptionBudgets, please refer to the official documentation.</p> <p>See also:</p> <ul> <li>replicaCount</li> </ul>"},{"location":"start/values/podextralabels.html","title":"podExtraLabels","text":"<p>Feature state:  1.1.0</p> <p><code>podExtraLabels</code> value in Kubedeploy can be used to define additional Pod labels</p> <p>Note</p> <p>By default no extra <code>podExtraLabels</code> are defined.</p> <p>Define podExtraLabels</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\npodExtraLabels:\nmy-component: \"user-app\"\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the above example all deployed objects will get extra label <code>my-component: \"user-app\"</code>.</p> <p>See also:</p> <ul> <li>podAnnotations</li> </ul>"},{"location":"start/values/podsecuritycontext.html","title":"podSecurityContext","text":"<p><code>podSecurityContext</code> value in Kubedeploy can be used to define raw Pod level securityContext.</p> <p>Note</p> <p>By default <code>podSecurityContext</code> is undefined.</p> <p>Define podSecurityContext</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\npodSecurityContext:\nfsGroup: 2000\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>See also:</p> <ul> <li>securityContext</li> <li>Best practices</li> </ul>"},{"location":"start/values/ports.html","title":"ports","text":"<p><code>ports</code> value in Kubedeploy can be used to define main container exposed ports. User can define container exposed ports see: containerPort for example</p> <p>Note</p> <p>By default container <code>ports</code> are undefined.</p> <p>As bare minimum, ports should take a list of contarinerPort, however user can name the ports and define port protocol.</p> <p>Define main container exposed ports</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nports:\n# minimal configuration\n- containerPort: 80\n# extended configuration\n- contarinerPort: 8080\nname: extrahttp\nprotocol: TCP\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Note</p> <p><code>additionalContainer</code> can define their own exposed ports, for more info see the related configuration values</p> <p>Note</p> <p><code>service</code> and <code>ingress</code> will require at least one exposed port to properly route traffic to your application. For more info see the related configuration values.</p> <p>See also:</p> <ul> <li>additionalContainers</li> <li>service</li> <li>ingress</li> </ul>"},{"location":"start/values/replicacount.html","title":"replicaCount","text":"<p><code>replicaCount</code> value in Kubedeploy will adjust number of Pod replicas for your application in the cluster, enhancing the application's resiliency and throughput, if other complimentary configurations are properly set.</p> <p>By default <code>replicaCount</code> is set to <code>1</code>.</p> <p>Application with 3 replicas</p> values.yaml<pre><code>image:\nrepository: nginx\ntag: 1.25.2\nreplicaCount: 3\n</code></pre> Deploy command<pre><code>helm install webserver sysbee/kubedeploy -f values.yaml\n</code></pre> <p>replicaCount != High Availability</p> <p>It's important to note that increasing the replicaCount does not automatically guarantee high availability. The Kubernets scheduler might place all the Pod replicas on single node if Pod <code>AntiAffinity</code> is not configured, resulting in downtime if that specific Kubernetes node goes offline.</p> <p>See also:</p> <ul> <li>podAntiAffinity</li> <li>affinity</li> <li>topologySpreadConstraints</li> </ul>"},{"location":"start/values/resources.html","title":"resources","text":"<p><code>resources</code> value in Kubedeploy can be used to define main container resource requirements.</p> <p>By default <code>resources</code> are intentionally left undefined in Kubedeploy, as this should be a conscious choice for the user. For guide on how to properly select resource requirements, see the Best practices Resource requirements section</p> <p>Define resource requirements for the main container</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nresources:\nrequests:\ncpu: 0.1\nmemory: 128Mi\nlimits:\ncpu: 1\nmemory: 512Mi\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Note</p> <p><code>initContainers</code> and <code>additionalContainer</code> define their own resource requirements, for more info see the related configuration values</p> <p>See also:</p> <ul> <li>initContainers</li> <li>additionalContainers</li> </ul>"},{"location":"start/values/securitycontext.html","title":"securityContext","text":"<p><code>securityContext</code> value in Kubedeploy can be used to define raw main containers securityContext.</p> <p>Note</p> <p>By default <code>securityContext</code> is undefined.</p> <p>Define securityContext</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nsecurityContext:\ncapabilities:\ndrop:\n- ALL\nreadOnlyRootFilesystem: true\nrunAsNonRoot: true\nrunAsUser: 1000\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Note</p> <p>Unlike <code>podSecurityContext</code>, defining <code>securityContext</code> will be applied only to main container.</p> <p>See also:</p> <ul> <li>podSecurityContext</li> <li>Best practices</li> </ul>"},{"location":"start/values/service.html","title":"service","text":"<p><code>service</code> value in Kubedeploy allows you to define parametars for Service object that would be created.</p> <p>In Kubernetes, a Service is a method for exposing a network application that is running as one or more Pods in your cluster.</p> <p>A key aim of Services in Kubernetes is that you don't need to modify your existing application to use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a code designed for a cloud-native world, or an older app you've containerized. You use a Service to make that set of Pods available on the network so that clients can interact with it.</p> <p>Note</p> <p>By default <code>service</code> is enabled without any service ports defined. If <code>service.ports</code> is undefined Kubedeploy will generate a ports list based on the exposed main container ports using <code>ports</code> value.</p> <p>Additional containers ports will not be included in this automatically generated list of service ports.</p> <p>Info</p> <p>To limit exposed ports via Service object, you can define custom <code>service.ports</code> list.</p> <p>Available values for <code>service</code> in Kubedeploy:</p> <pre><code>service:\ntype: ClusterIP # (1)\nenabled: true # (2)\nheadless: false # (3)\nports: [] # (4)\n</code></pre> <ol> <li>Set Service type. See Service for available options.</li> <li>Enable Service provisioning for release.</li> <li> <p>(Feature state:  1.1.0) Create a headless service. See reference</p> </li> <li> <p>(list) Define listening ports for Service.     If unspecified, chart will automatically generate ports list based on main container exposed ports.     Example values in ports list:</p> <p>Example</p> <pre><code>service:\nports:\n- port: 80 # (required) - port number as integer\ntargetPort: http # (required) - name of the exposed container or additionalContainer port as string\nprotocol: TCP # (required) Define service protocol. See [Supported protocols](https://kubernetes.io/docs/concepts/services-networking/service/#protocol-support)\nname: http # (required) - name of the Service port as string. This should match container's port name\n</code></pre> </li> </ol> <p>Simple configuration exposing all the container ports:</p> <p>Default setup</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\nports:\n- containerPort: 80\nname: http\n- containerPort: 8080\nname: metrics\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>Service is always enabled so there is no need to specifically define any values for it. With the above values, Kubedeploy creates service object named <code>webapp-myapp</code> that will be listening on same ports as containerPorts: <code>80</code> and <code>8080</code>. Service object will load balance requests between three application replicas.</p> <p>Disabling service object</p> <p>If you don't wish to deploy Service object it only requires adding the</p> <p><pre><code>service:\nenabled: false\n</code></pre> to the previous example</p> <p>Building on the previous example, if we only want to expose http port via Service object and leave the metrics port available only when contacting each pod directly we can simply define custom ports in <code>service</code> values</p> <p>Define custom service object</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 3\nports:\n- containerPort: 80\nname: http\n- containerPort: 8080\nname: metrics\nservice:\nports:\n- port: 80\ntargetPort: http # (1)\nprotocol: TCP\nname: http # (2)\n</code></pre> <ol> <li>targetPort is the name given to the containerPorts exposed via <code>ports</code> values</li> <li>name is the port name in Service object, and it should match the <code>targetPort</code> name</li> </ol> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In this example Service object will only Load balance port <code>80</code> between application replicas.</p> <p>See also:</p> <ul> <li>ports</li> <li>ingress</li> </ul>"},{"location":"start/values/serviceaccount.html","title":"serviceAccount","text":"<p><code>serviceAccount</code> value in Kubedeploy controls if new service account should be created or if the application will use existing one.</p> <p>Note</p> <p>By default <code>serviceAccount</code> creation is enabled.</p> <p>Annotate created service account with IRSA</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-role\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>With the example above created service account will have extra annotatins required to get the IRSA working for your application. Kubdeploy will not however deploy IRSA configuration on your cluster or AWS account.</p> <p>Info</p> <p>Find out more about IRSA</p>"},{"location":"start/values/tolerations.html","title":"tolerations","text":"<p>Nodes can be grouped for provisioning, isolating workloads from one another. For example, Kubernetes cluster might have a dedicated node group solely for database services. To achieve this, nodes can be configured with specific node taints. Pods that lack specified tolerations for those taints will avoid scheduling on such node groups.</p> <p>For more information on toleration and taints, please follow the official documentation.</p> <p>Kubedeploy offers the ability to define custom tolerations for Pods with <code>tolerations</code> value configuration.</p> <p>Note</p> <p>By default <code>tolerations</code> is undefined.</p> <p>Let's assume we have a specific node group running the <code>c6</code> instance family, intended only for database workloads. Nodes have a specific label set: <code>workload-type: database</code>.</p> <p>Additionally, there's a taint  named <code>workload-type</code> with the value <code>database</code> and the effect <code>NoSchedule</code>. If we wish to target this specific node group, we can't rely solely on Karpenter's known labels. Using those labels would provision new nodes of the desired instance family, where other workloads could be scheduled if free capacity is available.</p> <p>To target this specific group, we need to use a <code>nodeSelector</code> with <code>tolerations</code>.</p> <p>Define custom tolerations</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nnodeSelector:\nworkload-type: database\ntolerations:\n- key: workload-type\nvalue: database\neffect: NoSchedule\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>This configuration targets the specific <code>workload-type: database</code> node group. Kubernets cluster administrators can then control which instance family is required for this workload on a system level, rather than defining the desired instance family in each deployment. The toleration key ensures that our Pods will tolerate the taints set on the node group.</p> <p>See also:</p> <ul> <li>podAntiAffinity</li> <li>podAntiAffinityTopologyKey</li> <li>affinity</li> <li>nodeSelector</li> <li>topologySpreadConstraints</li> <li>Best practices</li> </ul>"},{"location":"start/values/topologyspreadconstraints.html","title":"topologySpreadConstraints","text":"<p><code>topologySpreadConstraints</code> value enables defining of raw topologySpreadConstraings for Pod.</p> <p>Note</p> <p>By default <code>topologySpreadConstraints</code> is undefined.</p> <p>In essence, these constraints provide a flexible alternative to Pod Affinity/Anti-Affinity. Topology spread constraints let you separate nodes into groups and assign Pods using a label selector. They also allow you to instruct the scheduler on how (un)evenly to distribute those Pods.</p> <p>Topology spread constraints can overlap with other scheduling policies like Node Selector or taints.</p> <p>Define custom topologySpreadConstraints rules</p> values.yaml<pre><code>nameOverride: my-app\nimage:\nrepository: nginx\ntag: latest\nreplicaCount: 5\ntoplologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: kubernetes.io/zone\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name=my-app\n</code></pre> Deploy command<pre><code>helm install webapp sysbee/kubedeploy -f values.yaml\n</code></pre> <p>In the example above, with cluster deployed in only two availability zones, three Pods would be deployed in zone <code>A</code> and two Pods in zone <code>B</code>. See the Best practices link below for more info.</p> <p>For more examples on custom topologySpreadCOnstraints, please read the official documentation.</p> <p>See also:</p> <ul> <li>podAntiAffinity</li> <li>podAntiAffinityTopologyKey</li> <li>affinity</li> <li>nodeSelector</li> <li>Best practices</li> </ul>"}]}